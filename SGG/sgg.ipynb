{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "object_map = {0: \"bicycle\", 1: \"bus\", 2: \"crosswalk\", 3: \"pedestrian\", 4: \"pedestrian sign\", 5: \"stop sign\", 6: \"tactile paving\", 7: \"traffic light\", 8: \"truck\", 9: \"car\", 10: \"scooter\", 11: \"motorcycle\"}\n",
    "object_list = list(object_map.values())\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "output_embeddings = []\n",
    "for obj in object_list:\n",
    "    encoded_input = tokenizer(obj, return_tensors='tf')\n",
    "    output = model(encoded_input)\n",
    "    output_embeddings.append(torch.Tensor(output.last_hidden_state.numpy().sum(axis=1).squeeze(0)))\n",
    "    \n",
    "file_name = 'vocab_embeddings.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(output_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "try:\n",
    "    with open('vocab_embeddings.pkl', 'rb') as file:\n",
    "        output_embeddings = pickle.load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"The file was not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = 'sgg_data.csv'\n",
    "features = []\n",
    "images = []\n",
    "labels = []\n",
    "with open(csv_file, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        images.append(row[0])\n",
    "        labels.append(int(row[-1]))\n",
    "        bb1 = [float(item) for item in row[1:5]]\n",
    "        bb2 = [float(item) for item in row[6:10]]\n",
    "        diff_bb = [float(bb1[i]) - float(bb2[i]) for i in range(len(bb1))]\n",
    "        features.append([int(row[5])] + [int(row[10])] + bb1 + bb2 + diff_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = []\n",
    "for i in range(len(features)):\n",
    "    X.append(torch.cat((torch.Tensor(features[i][2:]), output_embeddings[features[i][0]], output_embeddings[features[i][1]], output_embeddings[features[i][0]] - output_embeddings[features[i][1]])))\n",
    "X = torch.stack(X)\n",
    "Y = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = []\n",
    "object_classes_len = 12\n",
    "for i in range(len(features)):\n",
    "    X.append(torch.cat((torch.Tensor(features[i][2:]), torch.nn.functional.one_hot(torch.tensor(features[i][0]), num_classes=object_classes_len), torch.nn.functional.one_hot(torch.tensor(features[i][0]), num_classes=object_classes_len))))\n",
    "X = torch.stack(X)\n",
    "Y = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4414,  0.1630,  0.4898,  0.3667,  0.5753,  0.1508,  0.6117,  0.3593,\n",
       "        -0.1339,  0.0122, -0.1219,  0.0074,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "u1k--jLDzNO8"
   },
   "outputs": [],
   "source": [
    "from SceneGraph import SceneGraph\n",
    "def create_model(num_classes, num_features):\n",
    "    model = SceneGraph(num_classes, num_features)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "8KgjEKkTzS_7"
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, features, labels):\n",
    "    model.train()\n",
    "\n",
    "    features, labels = Variable(features), Variable(labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "yOvVcBhfzV6a"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, features, labels):\n",
    "    model.eval()\n",
    "\n",
    "    features, labels = Variable(features), Variable(labels)\n",
    "\n",
    "    outputs = model(features)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = labels.size(0)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "KHfGN5WWzZcs"
   },
   "outputs": [],
   "source": [
    "model = create_model(num_classes=16, num_features=X.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "NaTqtlirzceM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000 | Training Loss: 2.7790 | Validation Loss: 2.7266\n",
      "Epoch: 2/1000 | Training Loss: 2.7240 | Validation Loss: 2.6823\n",
      "Epoch: 3/1000 | Training Loss: 2.6712 | Validation Loss: 2.6372\n",
      "Epoch: 4/1000 | Training Loss: 2.6167 | Validation Loss: 2.5906\n",
      "Epoch: 5/1000 | Training Loss: 2.5594 | Validation Loss: 2.5436\n",
      "Epoch: 6/1000 | Training Loss: 2.5008 | Validation Loss: 2.4985\n",
      "Epoch: 7/1000 | Training Loss: 2.4429 | Validation Loss: 2.4590\n",
      "Epoch: 8/1000 | Training Loss: 2.3894 | Validation Loss: 2.4289\n",
      "Epoch: 9/1000 | Training Loss: 2.3440 | Validation Loss: 2.4099\n",
      "Epoch: 10/1000 | Training Loss: 2.3097 | Validation Loss: 2.4012\n",
      "Epoch: 11/1000 | Training Loss: 2.2867 | Validation Loss: 2.3983\n",
      "Epoch: 12/1000 | Training Loss: 2.2715 | Validation Loss: 2.3951\n",
      "Epoch: 13/1000 | Training Loss: 2.2585 | Validation Loss: 2.3876\n",
      "Epoch: 14/1000 | Training Loss: 2.2434 | Validation Loss: 2.3750\n",
      "Epoch: 15/1000 | Training Loss: 2.2247 | Validation Loss: 2.3589\n",
      "Epoch: 16/1000 | Training Loss: 2.2035 | Validation Loss: 2.3415\n",
      "Epoch: 17/1000 | Training Loss: 2.1816 | Validation Loss: 2.3241\n",
      "Epoch: 18/1000 | Training Loss: 2.1600 | Validation Loss: 2.3072\n",
      "Epoch: 19/1000 | Training Loss: 2.1395 | Validation Loss: 2.2911\n",
      "Epoch: 20/1000 | Training Loss: 2.1199 | Validation Loss: 2.2754\n",
      "Epoch: 21/1000 | Training Loss: 2.1012 | Validation Loss: 2.2600\n",
      "Epoch: 22/1000 | Training Loss: 2.0831 | Validation Loss: 2.2454\n",
      "Epoch: 23/1000 | Training Loss: 2.0656 | Validation Loss: 2.2316\n",
      "Epoch: 24/1000 | Training Loss: 2.0487 | Validation Loss: 2.2193\n",
      "Epoch: 25/1000 | Training Loss: 2.0326 | Validation Loss: 2.2088\n",
      "Epoch: 26/1000 | Training Loss: 2.0174 | Validation Loss: 2.2000\n",
      "Epoch: 27/1000 | Training Loss: 2.0028 | Validation Loss: 2.1931\n",
      "Epoch: 28/1000 | Training Loss: 1.9884 | Validation Loss: 2.1878\n",
      "Epoch: 29/1000 | Training Loss: 1.9742 | Validation Loss: 2.1838\n",
      "Epoch: 30/1000 | Training Loss: 1.9601 | Validation Loss: 2.1808\n",
      "Epoch: 31/1000 | Training Loss: 1.9463 | Validation Loss: 2.1783\n",
      "Epoch: 32/1000 | Training Loss: 1.9328 | Validation Loss: 2.1758\n",
      "Epoch: 33/1000 | Training Loss: 1.9198 | Validation Loss: 2.1727\n",
      "Epoch: 34/1000 | Training Loss: 1.9072 | Validation Loss: 2.1686\n",
      "Epoch: 35/1000 | Training Loss: 1.8949 | Validation Loss: 2.1636\n",
      "Epoch: 36/1000 | Training Loss: 1.8827 | Validation Loss: 2.1575\n",
      "Epoch: 37/1000 | Training Loss: 1.8709 | Validation Loss: 2.1505\n",
      "Epoch: 38/1000 | Training Loss: 1.8593 | Validation Loss: 2.1427\n",
      "Epoch: 39/1000 | Training Loss: 1.8481 | Validation Loss: 2.1341\n",
      "Epoch: 40/1000 | Training Loss: 1.8369 | Validation Loss: 2.1247\n",
      "Epoch: 41/1000 | Training Loss: 1.8257 | Validation Loss: 2.1147\n",
      "Epoch: 42/1000 | Training Loss: 1.8143 | Validation Loss: 2.1045\n",
      "Epoch: 43/1000 | Training Loss: 1.8029 | Validation Loss: 2.0942\n",
      "Epoch: 44/1000 | Training Loss: 1.7916 | Validation Loss: 2.0843\n",
      "Epoch: 45/1000 | Training Loss: 1.7806 | Validation Loss: 2.0751\n",
      "Epoch: 46/1000 | Training Loss: 1.7697 | Validation Loss: 2.0669\n",
      "Epoch: 47/1000 | Training Loss: 1.7588 | Validation Loss: 2.0600\n",
      "Epoch: 48/1000 | Training Loss: 1.7479 | Validation Loss: 2.0543\n",
      "Epoch: 49/1000 | Training Loss: 1.7369 | Validation Loss: 2.0499\n",
      "Epoch: 50/1000 | Training Loss: 1.7259 | Validation Loss: 2.0465\n",
      "Epoch: 51/1000 | Training Loss: 1.7151 | Validation Loss: 2.0436\n",
      "Epoch: 52/1000 | Training Loss: 1.7043 | Validation Loss: 2.0411\n",
      "Epoch: 53/1000 | Training Loss: 1.6936 | Validation Loss: 2.0388\n",
      "Epoch: 54/1000 | Training Loss: 1.6830 | Validation Loss: 2.0369\n",
      "Epoch: 55/1000 | Training Loss: 1.6725 | Validation Loss: 2.0347\n",
      "Epoch: 56/1000 | Training Loss: 1.6620 | Validation Loss: 2.0320\n",
      "Epoch: 57/1000 | Training Loss: 1.6515 | Validation Loss: 2.0286\n",
      "Epoch: 58/1000 | Training Loss: 1.6411 | Validation Loss: 2.0249\n",
      "Epoch: 59/1000 | Training Loss: 1.6309 | Validation Loss: 2.0212\n",
      "Epoch: 60/1000 | Training Loss: 1.6208 | Validation Loss: 2.0179\n",
      "Epoch: 61/1000 | Training Loss: 1.6109 | Validation Loss: 2.0151\n",
      "Epoch: 62/1000 | Training Loss: 1.6010 | Validation Loss: 2.0127\n",
      "Epoch: 63/1000 | Training Loss: 1.5912 | Validation Loss: 2.0104\n",
      "Epoch: 64/1000 | Training Loss: 1.5814 | Validation Loss: 2.0083\n",
      "Epoch: 65/1000 | Training Loss: 1.5718 | Validation Loss: 2.0065\n",
      "Epoch: 66/1000 | Training Loss: 1.5624 | Validation Loss: 2.0049\n",
      "Epoch: 67/1000 | Training Loss: 1.5530 | Validation Loss: 2.0034\n",
      "Epoch: 68/1000 | Training Loss: 1.5438 | Validation Loss: 2.0014\n",
      "Epoch: 69/1000 | Training Loss: 1.5347 | Validation Loss: 1.9991\n",
      "Epoch: 70/1000 | Training Loss: 1.5257 | Validation Loss: 1.9964\n",
      "Epoch: 71/1000 | Training Loss: 1.5168 | Validation Loss: 1.9938\n",
      "Epoch: 72/1000 | Training Loss: 1.5080 | Validation Loss: 1.9914\n",
      "Epoch: 73/1000 | Training Loss: 1.4993 | Validation Loss: 1.9888\n",
      "Epoch: 74/1000 | Training Loss: 1.4907 | Validation Loss: 1.9864\n",
      "Epoch: 75/1000 | Training Loss: 1.4822 | Validation Loss: 1.9846\n",
      "Epoch: 76/1000 | Training Loss: 1.4738 | Validation Loss: 1.9832\n",
      "Epoch: 77/1000 | Training Loss: 1.4654 | Validation Loss: 1.9820\n",
      "Epoch: 78/1000 | Training Loss: 1.4572 | Validation Loss: 1.9807\n",
      "Epoch: 79/1000 | Training Loss: 1.4490 | Validation Loss: 1.9793\n",
      "Epoch: 80/1000 | Training Loss: 1.4409 | Validation Loss: 1.9777\n",
      "Epoch: 81/1000 | Training Loss: 1.4329 | Validation Loss: 1.9762\n",
      "Epoch: 82/1000 | Training Loss: 1.4250 | Validation Loss: 1.9747\n",
      "Epoch: 83/1000 | Training Loss: 1.4171 | Validation Loss: 1.9734\n",
      "Epoch: 84/1000 | Training Loss: 1.4092 | Validation Loss: 1.9723\n",
      "Epoch: 85/1000 | Training Loss: 1.4015 | Validation Loss: 1.9712\n",
      "Epoch: 86/1000 | Training Loss: 1.3937 | Validation Loss: 1.9698\n",
      "Epoch: 87/1000 | Training Loss: 1.3861 | Validation Loss: 1.9688\n",
      "Epoch: 88/1000 | Training Loss: 1.3784 | Validation Loss: 1.9683\n",
      "Epoch: 89/1000 | Training Loss: 1.3708 | Validation Loss: 1.9680\n",
      "Epoch: 90/1000 | Training Loss: 1.3633 | Validation Loss: 1.9675\n",
      "Epoch: 91/1000 | Training Loss: 1.3558 | Validation Loss: 1.9670\n",
      "Epoch: 92/1000 | Training Loss: 1.3483 | Validation Loss: 1.9665\n",
      "Epoch: 93/1000 | Training Loss: 1.3409 | Validation Loss: 1.9656\n",
      "Epoch: 94/1000 | Training Loss: 1.3335 | Validation Loss: 1.9648\n",
      "Epoch: 95/1000 | Training Loss: 1.3262 | Validation Loss: 1.9642\n",
      "Epoch: 96/1000 | Training Loss: 1.3190 | Validation Loss: 1.9634\n",
      "Epoch: 97/1000 | Training Loss: 1.3118 | Validation Loss: 1.9634\n",
      "Epoch: 98/1000 | Training Loss: 1.3046 | Validation Loss: 1.9635\n",
      "Epoch: 99/1000 | Training Loss: 1.2975 | Validation Loss: 1.9630\n",
      "Epoch: 100/1000 | Training Loss: 1.2904 | Validation Loss: 1.9631\n",
      "Epoch: 101/1000 | Training Loss: 1.2834 | Validation Loss: 1.9628\n",
      "Epoch: 102/1000 | Training Loss: 1.2764 | Validation Loss: 1.9627\n",
      "Epoch: 103/1000 | Training Loss: 1.2695 | Validation Loss: 1.9634\n",
      "Epoch: 104/1000 | Training Loss: 1.2627 | Validation Loss: 1.9622\n",
      "Epoch: 105/1000 | Training Loss: 1.2559 | Validation Loss: 1.9622\n",
      "Epoch: 106/1000 | Training Loss: 1.2491 | Validation Loss: 1.9621\n",
      "Epoch: 107/1000 | Training Loss: 1.2423 | Validation Loss: 1.9620\n",
      "Epoch: 108/1000 | Training Loss: 1.2355 | Validation Loss: 1.9619\n",
      "Epoch: 109/1000 | Training Loss: 1.2288 | Validation Loss: 1.9607\n",
      "Epoch: 110/1000 | Training Loss: 1.2221 | Validation Loss: 1.9613\n",
      "Epoch: 111/1000 | Training Loss: 1.2154 | Validation Loss: 1.9595\n",
      "Epoch: 112/1000 | Training Loss: 1.2089 | Validation Loss: 1.9611\n",
      "Epoch: 113/1000 | Training Loss: 1.2025 | Validation Loss: 1.9563\n",
      "Epoch: 114/1000 | Training Loss: 1.1963 | Validation Loss: 1.9626\n",
      "Epoch: 115/1000 | Training Loss: 1.1904 | Validation Loss: 1.9563\n",
      "Epoch: 116/1000 | Training Loss: 1.1849 | Validation Loss: 1.9647\n",
      "Epoch: 117/1000 | Training Loss: 1.1789 | Validation Loss: 1.9573\n",
      "Epoch: 118/1000 | Training Loss: 1.1720 | Validation Loss: 1.9595\n",
      "Epoch: 119/1000 | Training Loss: 1.1654 | Validation Loss: 1.9634\n",
      "Epoch: 120/1000 | Training Loss: 1.1602 | Validation Loss: 1.9592\n",
      "Epoch: 121/1000 | Training Loss: 1.1551 | Validation Loss: 1.9649\n",
      "Epoch: 122/1000 | Training Loss: 1.1492 | Validation Loss: 1.9612\n",
      "Epoch: 123/1000 | Training Loss: 1.1430 | Validation Loss: 1.9625\n",
      "Epoch: 124/1000 | Training Loss: 1.1376 | Validation Loss: 1.9675\n",
      "Epoch: 125/1000 | Training Loss: 1.1328 | Validation Loss: 1.9638\n",
      "Epoch: 126/1000 | Training Loss: 1.1276 | Validation Loss: 1.9685\n",
      "Epoch: 127/1000 | Training Loss: 1.1219 | Validation Loss: 1.9680\n",
      "Epoch: 128/1000 | Training Loss: 1.1163 | Validation Loss: 1.9693\n",
      "Epoch: 129/1000 | Training Loss: 1.1114 | Validation Loss: 1.9741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130/1000 | Training Loss: 1.1067 | Validation Loss: 1.9717\n",
      "Epoch: 131/1000 | Training Loss: 1.1019 | Validation Loss: 1.9765\n",
      "Epoch: 132/1000 | Training Loss: 1.0967 | Validation Loss: 1.9761\n",
      "Epoch: 133/1000 | Training Loss: 1.0914 | Validation Loss: 1.9790\n",
      "Epoch: 134/1000 | Training Loss: 1.0865 | Validation Loss: 1.9821\n",
      "Epoch: 135/1000 | Training Loss: 1.0818 | Validation Loss: 1.9828\n",
      "Epoch: 136/1000 | Training Loss: 1.0774 | Validation Loss: 1.9886\n",
      "Epoch: 137/1000 | Training Loss: 1.0730 | Validation Loss: 1.9881\n",
      "Epoch: 138/1000 | Training Loss: 1.0685 | Validation Loss: 1.9940\n",
      "Epoch: 139/1000 | Training Loss: 1.0638 | Validation Loss: 1.9928\n",
      "Epoch: 140/1000 | Training Loss: 1.0590 | Validation Loss: 1.9967\n",
      "Epoch: 141/1000 | Training Loss: 1.0541 | Validation Loss: 1.9984\n",
      "Epoch: 142/1000 | Training Loss: 1.0494 | Validation Loss: 2.0007\n",
      "Epoch: 143/1000 | Training Loss: 1.0449 | Validation Loss: 2.0036\n",
      "Epoch: 144/1000 | Training Loss: 1.0406 | Validation Loss: 2.0052\n",
      "Epoch: 145/1000 | Training Loss: 1.0365 | Validation Loss: 2.0102\n",
      "Epoch: 146/1000 | Training Loss: 1.0326 | Validation Loss: 2.0102\n",
      "Epoch: 147/1000 | Training Loss: 1.0292 | Validation Loss: 2.0173\n",
      "Epoch: 148/1000 | Training Loss: 1.0262 | Validation Loss: 2.0162\n",
      "Epoch: 149/1000 | Training Loss: 1.0224 | Validation Loss: 2.0215\n",
      "Epoch: 150/1000 | Training Loss: 1.0172 | Validation Loss: 2.0208\n",
      "Epoch: 151/1000 | Training Loss: 1.0115 | Validation Loss: 2.0232\n",
      "Epoch: 152/1000 | Training Loss: 1.0075 | Validation Loss: 2.0286\n",
      "Epoch: 153/1000 | Training Loss: 1.0049 | Validation Loss: 2.0297\n",
      "Epoch: 154/1000 | Training Loss: 1.0018 | Validation Loss: 2.0344\n",
      "Epoch: 155/1000 | Training Loss: 0.9973 | Validation Loss: 2.0349\n",
      "Epoch: 156/1000 | Training Loss: 0.9923 | Validation Loss: 2.0396\n",
      "Epoch: 157/1000 | Training Loss: 0.9884 | Validation Loss: 2.0459\n",
      "Epoch: 158/1000 | Training Loss: 0.9856 | Validation Loss: 2.0474\n",
      "Epoch: 159/1000 | Training Loss: 0.9824 | Validation Loss: 2.0510\n",
      "Epoch: 160/1000 | Training Loss: 0.9783 | Validation Loss: 2.0527\n",
      "Epoch: 161/1000 | Training Loss: 0.9737 | Validation Loss: 2.0568\n",
      "Epoch: 162/1000 | Training Loss: 0.9699 | Validation Loss: 2.0618\n",
      "Epoch: 163/1000 | Training Loss: 0.9668 | Validation Loss: 2.0654\n",
      "Epoch: 164/1000 | Training Loss: 0.9637 | Validation Loss: 2.0704\n",
      "Epoch: 165/1000 | Training Loss: 0.9601 | Validation Loss: 2.0728\n",
      "Epoch: 166/1000 | Training Loss: 0.9561 | Validation Loss: 2.0774\n",
      "Epoch: 167/1000 | Training Loss: 0.9522 | Validation Loss: 2.0830\n",
      "Epoch: 168/1000 | Training Loss: 0.9487 | Validation Loss: 2.0876\n",
      "Epoch: 169/1000 | Training Loss: 0.9455 | Validation Loss: 2.0920\n",
      "Epoch: 170/1000 | Training Loss: 0.9424 | Validation Loss: 2.0959\n",
      "Epoch: 171/1000 | Training Loss: 0.9392 | Validation Loss: 2.1020\n",
      "Epoch: 172/1000 | Training Loss: 0.9357 | Validation Loss: 2.1061\n",
      "Epoch: 173/1000 | Training Loss: 0.9322 | Validation Loss: 2.1089\n",
      "Epoch: 174/1000 | Training Loss: 0.9286 | Validation Loss: 2.1117\n",
      "Epoch: 175/1000 | Training Loss: 0.9251 | Validation Loss: 2.1172\n",
      "Epoch: 176/1000 | Training Loss: 0.9216 | Validation Loss: 2.1226\n",
      "Epoch: 177/1000 | Training Loss: 0.9184 | Validation Loss: 2.1269\n",
      "Epoch: 178/1000 | Training Loss: 0.9151 | Validation Loss: 2.1308\n",
      "Epoch: 179/1000 | Training Loss: 0.9119 | Validation Loss: 2.1352\n",
      "Epoch: 180/1000 | Training Loss: 0.9088 | Validation Loss: 2.1401\n",
      "Epoch: 181/1000 | Training Loss: 0.9059 | Validation Loss: 2.1454\n",
      "Epoch: 182/1000 | Training Loss: 0.9034 | Validation Loss: 2.1514\n",
      "Epoch: 183/1000 | Training Loss: 0.9015 | Validation Loss: 2.1576\n",
      "Epoch: 184/1000 | Training Loss: 0.9001 | Validation Loss: 2.1645\n",
      "Epoch: 185/1000 | Training Loss: 0.8986 | Validation Loss: 2.1686\n",
      "Epoch: 186/1000 | Training Loss: 0.8940 | Validation Loss: 2.1710\n",
      "Epoch: 187/1000 | Training Loss: 0.8881 | Validation Loss: 2.1748\n",
      "Epoch: 188/1000 | Training Loss: 0.8841 | Validation Loss: 2.1820\n",
      "Epoch: 189/1000 | Training Loss: 0.8828 | Validation Loss: 2.1895\n",
      "Epoch: 190/1000 | Training Loss: 0.8818 | Validation Loss: 2.1938\n",
      "Epoch: 191/1000 | Training Loss: 0.8779 | Validation Loss: 2.1968\n",
      "Epoch: 192/1000 | Training Loss: 0.8729 | Validation Loss: 2.2027\n",
      "Epoch: 193/1000 | Training Loss: 0.8699 | Validation Loss: 2.2099\n",
      "Epoch: 194/1000 | Training Loss: 0.8686 | Validation Loss: 2.2144\n",
      "Epoch: 195/1000 | Training Loss: 0.8666 | Validation Loss: 2.2181\n",
      "Epoch: 196/1000 | Training Loss: 0.8626 | Validation Loss: 2.2222\n",
      "Epoch: 197/1000 | Training Loss: 0.8587 | Validation Loss: 2.2289\n",
      "Epoch: 198/1000 | Training Loss: 0.8563 | Validation Loss: 2.2359\n",
      "Epoch: 199/1000 | Training Loss: 0.8546 | Validation Loss: 2.2406\n",
      "Epoch: 200/1000 | Training Loss: 0.8520 | Validation Loss: 2.2451\n",
      "Epoch: 201/1000 | Training Loss: 0.8484 | Validation Loss: 2.2499\n",
      "Epoch: 202/1000 | Training Loss: 0.8451 | Validation Loss: 2.2555\n",
      "Epoch: 203/1000 | Training Loss: 0.8428 | Validation Loss: 2.2628\n",
      "Epoch: 204/1000 | Training Loss: 0.8407 | Validation Loss: 2.2683\n",
      "Epoch: 205/1000 | Training Loss: 0.8383 | Validation Loss: 2.2738\n",
      "Epoch: 206/1000 | Training Loss: 0.8351 | Validation Loss: 2.2778\n",
      "Epoch: 207/1000 | Training Loss: 0.8320 | Validation Loss: 2.2829\n",
      "Epoch: 208/1000 | Training Loss: 0.8293 | Validation Loss: 2.2894\n",
      "Epoch: 209/1000 | Training Loss: 0.8271 | Validation Loss: 2.2949\n",
      "Epoch: 210/1000 | Training Loss: 0.8249 | Validation Loss: 2.3002\n",
      "Epoch: 211/1000 | Training Loss: 0.8224 | Validation Loss: 2.3056\n",
      "Epoch: 212/1000 | Training Loss: 0.8197 | Validation Loss: 2.3121\n",
      "Epoch: 213/1000 | Training Loss: 0.8169 | Validation Loss: 2.3180\n",
      "Epoch: 214/1000 | Training Loss: 0.8141 | Validation Loss: 2.3238\n",
      "Epoch: 215/1000 | Training Loss: 0.8115 | Validation Loss: 2.3300\n",
      "Epoch: 216/1000 | Training Loss: 0.8091 | Validation Loss: 2.3352\n",
      "Epoch: 217/1000 | Training Loss: 0.8068 | Validation Loss: 2.3410\n",
      "Epoch: 218/1000 | Training Loss: 0.8045 | Validation Loss: 2.3461\n",
      "Epoch: 219/1000 | Training Loss: 0.8022 | Validation Loss: 2.3522\n",
      "Epoch: 220/1000 | Training Loss: 0.7999 | Validation Loss: 2.3580\n",
      "Epoch: 221/1000 | Training Loss: 0.7977 | Validation Loss: 2.3655\n",
      "Epoch: 222/1000 | Training Loss: 0.7952 | Validation Loss: 2.3720\n",
      "Epoch: 223/1000 | Training Loss: 0.7927 | Validation Loss: 2.3780\n",
      "Epoch: 224/1000 | Training Loss: 0.7901 | Validation Loss: 2.3828\n",
      "Epoch: 225/1000 | Training Loss: 0.7875 | Validation Loss: 2.3883\n",
      "Epoch: 226/1000 | Training Loss: 0.7848 | Validation Loss: 2.3943\n",
      "Epoch: 227/1000 | Training Loss: 0.7823 | Validation Loss: 2.4007\n",
      "Epoch: 228/1000 | Training Loss: 0.7798 | Validation Loss: 2.4075\n",
      "Epoch: 229/1000 | Training Loss: 0.7774 | Validation Loss: 2.4135\n",
      "Epoch: 230/1000 | Training Loss: 0.7750 | Validation Loss: 2.4198\n",
      "Epoch: 231/1000 | Training Loss: 0.7727 | Validation Loss: 2.4256\n",
      "Epoch: 232/1000 | Training Loss: 0.7704 | Validation Loss: 2.4315\n",
      "Epoch: 233/1000 | Training Loss: 0.7680 | Validation Loss: 2.4370\n",
      "Epoch: 234/1000 | Training Loss: 0.7657 | Validation Loss: 2.4433\n",
      "Epoch: 235/1000 | Training Loss: 0.7634 | Validation Loss: 2.4490\n",
      "Epoch: 236/1000 | Training Loss: 0.7612 | Validation Loss: 2.4563\n",
      "Epoch: 237/1000 | Training Loss: 0.7592 | Validation Loss: 2.4620\n",
      "Epoch: 238/1000 | Training Loss: 0.7576 | Validation Loss: 2.4696\n",
      "Epoch: 239/1000 | Training Loss: 0.7559 | Validation Loss: 2.4754\n",
      "Epoch: 240/1000 | Training Loss: 0.7540 | Validation Loss: 2.4812\n",
      "Epoch: 241/1000 | Training Loss: 0.7520 | Validation Loss: 2.4908\n",
      "Epoch: 242/1000 | Training Loss: 0.7525 | Validation Loss: 2.4994\n",
      "Epoch: 243/1000 | Training Loss: 0.7552 | Validation Loss: 2.5124\n",
      "Epoch: 244/1000 | Training Loss: 0.7578 | Validation Loss: 2.5115\n",
      "Epoch: 245/1000 | Training Loss: 0.7496 | Validation Loss: 2.5108\n",
      "Epoch: 246/1000 | Training Loss: 0.7418 | Validation Loss: 2.5184\n",
      "Epoch: 247/1000 | Training Loss: 0.7398 | Validation Loss: 2.5288\n",
      "Epoch: 248/1000 | Training Loss: 0.7410 | Validation Loss: 2.5352\n",
      "Epoch: 249/1000 | Training Loss: 0.7401 | Validation Loss: 2.5337\n",
      "Epoch: 250/1000 | Training Loss: 0.7339 | Validation Loss: 2.5401\n",
      "Epoch: 251/1000 | Training Loss: 0.7308 | Validation Loss: 2.5521\n",
      "Epoch: 252/1000 | Training Loss: 0.7328 | Validation Loss: 2.5558\n",
      "Epoch: 253/1000 | Training Loss: 0.7302 | Validation Loss: 2.5576\n",
      "Epoch: 254/1000 | Training Loss: 0.7246 | Validation Loss: 2.5641\n",
      "Epoch: 255/1000 | Training Loss: 0.7240 | Validation Loss: 2.5699\n",
      "Epoch: 256/1000 | Training Loss: 0.7240 | Validation Loss: 2.5769\n",
      "Epoch: 257/1000 | Training Loss: 0.7206 | Validation Loss: 2.5839\n",
      "Epoch: 258/1000 | Training Loss: 0.7177 | Validation Loss: 2.5870\n",
      "Epoch: 259/1000 | Training Loss: 0.7164 | Validation Loss: 2.5941\n",
      "Epoch: 260/1000 | Training Loss: 0.7153 | Validation Loss: 2.6015\n",
      "Epoch: 261/1000 | Training Loss: 0.7131 | Validation Loss: 2.6050\n",
      "Epoch: 262/1000 | Training Loss: 0.7101 | Validation Loss: 2.6111\n",
      "Epoch: 263/1000 | Training Loss: 0.7089 | Validation Loss: 2.6206\n",
      "Epoch: 264/1000 | Training Loss: 0.7080 | Validation Loss: 2.6235\n",
      "Epoch: 265/1000 | Training Loss: 0.7054 | Validation Loss: 2.6252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 266/1000 | Training Loss: 0.7030 | Validation Loss: 2.6351\n",
      "Epoch: 267/1000 | Training Loss: 0.7017 | Validation Loss: 2.6444\n",
      "Epoch: 268/1000 | Training Loss: 0.7003 | Validation Loss: 2.6487\n",
      "Epoch: 269/1000 | Training Loss: 0.6984 | Validation Loss: 2.6567\n",
      "Epoch: 270/1000 | Training Loss: 0.6962 | Validation Loss: 2.6626\n",
      "Epoch: 271/1000 | Training Loss: 0.6944 | Validation Loss: 2.6659\n",
      "Epoch: 272/1000 | Training Loss: 0.6931 | Validation Loss: 2.6745\n",
      "Epoch: 273/1000 | Training Loss: 0.6914 | Validation Loss: 2.6812\n",
      "Epoch: 274/1000 | Training Loss: 0.6893 | Validation Loss: 2.6859\n",
      "Epoch: 275/1000 | Training Loss: 0.6875 | Validation Loss: 2.6931\n",
      "Epoch: 276/1000 | Training Loss: 0.6859 | Validation Loss: 2.6979\n",
      "Epoch: 277/1000 | Training Loss: 0.6843 | Validation Loss: 2.7033\n",
      "Epoch: 278/1000 | Training Loss: 0.6827 | Validation Loss: 2.7118\n",
      "Epoch: 279/1000 | Training Loss: 0.6809 | Validation Loss: 2.7174\n",
      "Epoch: 280/1000 | Training Loss: 0.6790 | Validation Loss: 2.7235\n",
      "Epoch: 281/1000 | Training Loss: 0.6774 | Validation Loss: 2.7314\n",
      "Epoch: 282/1000 | Training Loss: 0.6758 | Validation Loss: 2.7353\n",
      "Epoch: 283/1000 | Training Loss: 0.6741 | Validation Loss: 2.7405\n",
      "Epoch: 284/1000 | Training Loss: 0.6725 | Validation Loss: 2.7477\n",
      "Epoch: 285/1000 | Training Loss: 0.6709 | Validation Loss: 2.7527\n",
      "Epoch: 286/1000 | Training Loss: 0.6692 | Validation Loss: 2.7592\n",
      "Epoch: 287/1000 | Training Loss: 0.6675 | Validation Loss: 2.7648\n",
      "Epoch: 288/1000 | Training Loss: 0.6658 | Validation Loss: 2.7695\n",
      "Epoch: 289/1000 | Training Loss: 0.6642 | Validation Loss: 2.7773\n",
      "Epoch: 290/1000 | Training Loss: 0.6625 | Validation Loss: 2.7827\n",
      "Epoch: 291/1000 | Training Loss: 0.6609 | Validation Loss: 2.7880\n",
      "Epoch: 292/1000 | Training Loss: 0.6593 | Validation Loss: 2.7950\n",
      "Epoch: 293/1000 | Training Loss: 0.6577 | Validation Loss: 2.8010\n",
      "Epoch: 294/1000 | Training Loss: 0.6561 | Validation Loss: 2.8088\n",
      "Epoch: 295/1000 | Training Loss: 0.6545 | Validation Loss: 2.8133\n",
      "Epoch: 296/1000 | Training Loss: 0.6530 | Validation Loss: 2.8174\n",
      "Epoch: 297/1000 | Training Loss: 0.6515 | Validation Loss: 2.8255\n",
      "Epoch: 298/1000 | Training Loss: 0.6500 | Validation Loss: 2.8305\n",
      "Epoch: 299/1000 | Training Loss: 0.6485 | Validation Loss: 2.8381\n",
      "Epoch: 300/1000 | Training Loss: 0.6471 | Validation Loss: 2.8448\n",
      "Epoch: 301/1000 | Training Loss: 0.6457 | Validation Loss: 2.8508\n",
      "Epoch: 302/1000 | Training Loss: 0.6445 | Validation Loss: 2.8584\n",
      "Epoch: 303/1000 | Training Loss: 0.6430 | Validation Loss: 2.8661\n",
      "Epoch: 304/1000 | Training Loss: 0.6417 | Validation Loss: 2.8696\n",
      "Epoch: 305/1000 | Training Loss: 0.6401 | Validation Loss: 2.8773\n",
      "Epoch: 306/1000 | Training Loss: 0.6386 | Validation Loss: 2.8814\n",
      "Epoch: 307/1000 | Training Loss: 0.6367 | Validation Loss: 2.8909\n",
      "Epoch: 308/1000 | Training Loss: 0.6347 | Validation Loss: 2.8969\n",
      "Epoch: 309/1000 | Training Loss: 0.6327 | Validation Loss: 2.9017\n",
      "Epoch: 310/1000 | Training Loss: 0.6309 | Validation Loss: 2.9071\n",
      "Epoch: 311/1000 | Training Loss: 0.6293 | Validation Loss: 2.9117\n",
      "Epoch: 312/1000 | Training Loss: 0.6278 | Validation Loss: 2.9198\n",
      "Epoch: 313/1000 | Training Loss: 0.6266 | Validation Loss: 2.9269\n",
      "Epoch: 314/1000 | Training Loss: 0.6255 | Validation Loss: 2.9341\n",
      "Epoch: 315/1000 | Training Loss: 0.6247 | Validation Loss: 2.9424\n",
      "Epoch: 316/1000 | Training Loss: 0.6240 | Validation Loss: 2.9509\n",
      "Epoch: 317/1000 | Training Loss: 0.6237 | Validation Loss: 2.9544\n",
      "Epoch: 318/1000 | Training Loss: 0.6227 | Validation Loss: 2.9637\n",
      "Epoch: 319/1000 | Training Loss: 0.6214 | Validation Loss: 2.9635\n",
      "Epoch: 320/1000 | Training Loss: 0.6185 | Validation Loss: 2.9708\n",
      "Epoch: 321/1000 | Training Loss: 0.6155 | Validation Loss: 2.9755\n",
      "Epoch: 322/1000 | Training Loss: 0.6130 | Validation Loss: 2.9812\n",
      "Epoch: 323/1000 | Training Loss: 0.6115 | Validation Loss: 2.9903\n",
      "Epoch: 324/1000 | Training Loss: 0.6109 | Validation Loss: 2.9950\n",
      "Epoch: 325/1000 | Training Loss: 0.6103 | Validation Loss: 3.0047\n",
      "Epoch: 326/1000 | Training Loss: 0.6097 | Validation Loss: 3.0118\n",
      "Epoch: 327/1000 | Training Loss: 0.6081 | Validation Loss: 3.0179\n",
      "Epoch: 328/1000 | Training Loss: 0.6060 | Validation Loss: 3.0222\n",
      "Epoch: 329/1000 | Training Loss: 0.6034 | Validation Loss: 3.0271\n",
      "Epoch: 330/1000 | Training Loss: 0.6014 | Validation Loss: 3.0305\n",
      "Epoch: 331/1000 | Training Loss: 0.6000 | Validation Loss: 3.0371\n",
      "Epoch: 332/1000 | Training Loss: 0.5992 | Validation Loss: 3.0448\n",
      "Epoch: 333/1000 | Training Loss: 0.5986 | Validation Loss: 3.0513\n",
      "Epoch: 334/1000 | Training Loss: 0.5976 | Validation Loss: 3.0613\n",
      "Epoch: 335/1000 | Training Loss: 0.5964 | Validation Loss: 3.0636\n",
      "Epoch: 336/1000 | Training Loss: 0.5946 | Validation Loss: 3.0723\n",
      "Epoch: 337/1000 | Training Loss: 0.5926 | Validation Loss: 3.0758\n",
      "Epoch: 338/1000 | Training Loss: 0.5904 | Validation Loss: 3.0825\n",
      "Epoch: 339/1000 | Training Loss: 0.5887 | Validation Loss: 3.0919\n",
      "Epoch: 340/1000 | Training Loss: 0.5874 | Validation Loss: 3.0967\n",
      "Epoch: 341/1000 | Training Loss: 0.5864 | Validation Loss: 3.1082\n",
      "Epoch: 342/1000 | Training Loss: 0.5854 | Validation Loss: 3.1101\n",
      "Epoch: 343/1000 | Training Loss: 0.5843 | Validation Loss: 3.1193\n",
      "Epoch: 344/1000 | Training Loss: 0.5833 | Validation Loss: 3.1235\n",
      "Epoch: 345/1000 | Training Loss: 0.5821 | Validation Loss: 3.1286\n",
      "Epoch: 346/1000 | Training Loss: 0.5807 | Validation Loss: 3.1360\n",
      "Epoch: 347/1000 | Training Loss: 0.5790 | Validation Loss: 3.1424\n",
      "Epoch: 348/1000 | Training Loss: 0.5773 | Validation Loss: 3.1464\n",
      "Epoch: 349/1000 | Training Loss: 0.5757 | Validation Loss: 3.1526\n",
      "Epoch: 350/1000 | Training Loss: 0.5743 | Validation Loss: 3.1582\n",
      "Epoch: 351/1000 | Training Loss: 0.5728 | Validation Loss: 3.1656\n",
      "Epoch: 352/1000 | Training Loss: 0.5713 | Validation Loss: 3.1742\n",
      "Epoch: 353/1000 | Training Loss: 0.5699 | Validation Loss: 3.1776\n",
      "Epoch: 354/1000 | Training Loss: 0.5688 | Validation Loss: 3.1881\n",
      "Epoch: 355/1000 | Training Loss: 0.5677 | Validation Loss: 3.1889\n",
      "Epoch: 356/1000 | Training Loss: 0.5666 | Validation Loss: 3.1996\n",
      "Epoch: 357/1000 | Training Loss: 0.5657 | Validation Loss: 3.2055\n",
      "Epoch: 358/1000 | Training Loss: 0.5649 | Validation Loss: 3.2123\n",
      "Epoch: 359/1000 | Training Loss: 0.5644 | Validation Loss: 3.2164\n",
      "Epoch: 360/1000 | Training Loss: 0.5634 | Validation Loss: 3.2251\n",
      "Epoch: 361/1000 | Training Loss: 0.5625 | Validation Loss: 3.2295\n",
      "Epoch: 362/1000 | Training Loss: 0.5605 | Validation Loss: 3.2392\n",
      "Epoch: 363/1000 | Training Loss: 0.5586 | Validation Loss: 3.2386\n",
      "Epoch: 364/1000 | Training Loss: 0.5566 | Validation Loss: 3.2486\n",
      "Epoch: 365/1000 | Training Loss: 0.5548 | Validation Loss: 3.2509\n",
      "Epoch: 366/1000 | Training Loss: 0.5535 | Validation Loss: 3.2609\n",
      "Epoch: 367/1000 | Training Loss: 0.5525 | Validation Loss: 3.2685\n",
      "Epoch: 368/1000 | Training Loss: 0.5517 | Validation Loss: 3.2701\n",
      "Epoch: 369/1000 | Training Loss: 0.5508 | Validation Loss: 3.2827\n",
      "Epoch: 370/1000 | Training Loss: 0.5500 | Validation Loss: 3.2837\n",
      "Epoch: 371/1000 | Training Loss: 0.5489 | Validation Loss: 3.2957\n",
      "Epoch: 372/1000 | Training Loss: 0.5481 | Validation Loss: 3.2971\n",
      "Epoch: 373/1000 | Training Loss: 0.5466 | Validation Loss: 3.3064\n",
      "Epoch: 374/1000 | Training Loss: 0.5450 | Validation Loss: 3.3084\n",
      "Epoch: 375/1000 | Training Loss: 0.5428 | Validation Loss: 3.3133\n",
      "Epoch: 376/1000 | Training Loss: 0.5409 | Validation Loss: 3.3181\n",
      "Epoch: 377/1000 | Training Loss: 0.5396 | Validation Loss: 3.3251\n",
      "Epoch: 378/1000 | Training Loss: 0.5387 | Validation Loss: 3.3346\n",
      "Epoch: 379/1000 | Training Loss: 0.5380 | Validation Loss: 3.3399\n",
      "Epoch: 380/1000 | Training Loss: 0.5372 | Validation Loss: 3.3484\n",
      "Epoch: 381/1000 | Training Loss: 0.5363 | Validation Loss: 3.3492\n",
      "Epoch: 382/1000 | Training Loss: 0.5352 | Validation Loss: 3.3613\n",
      "Epoch: 383/1000 | Training Loss: 0.5342 | Validation Loss: 3.3610\n",
      "Epoch: 384/1000 | Training Loss: 0.5330 | Validation Loss: 3.3723\n",
      "Epoch: 385/1000 | Training Loss: 0.5319 | Validation Loss: 3.3752\n",
      "Epoch: 386/1000 | Training Loss: 0.5305 | Validation Loss: 3.3816\n",
      "Epoch: 387/1000 | Training Loss: 0.5291 | Validation Loss: 3.3883\n",
      "Epoch: 388/1000 | Training Loss: 0.5274 | Validation Loss: 3.3939\n",
      "Epoch: 389/1000 | Training Loss: 0.5257 | Validation Loss: 3.4007\n",
      "Epoch: 390/1000 | Training Loss: 0.5242 | Validation Loss: 3.4062\n",
      "Epoch: 391/1000 | Training Loss: 0.5231 | Validation Loss: 3.4136\n",
      "Epoch: 392/1000 | Training Loss: 0.5222 | Validation Loss: 3.4241\n",
      "Epoch: 393/1000 | Training Loss: 0.5214 | Validation Loss: 3.4315\n",
      "Epoch: 394/1000 | Training Loss: 0.5206 | Validation Loss: 3.4378\n",
      "Epoch: 395/1000 | Training Loss: 0.5197 | Validation Loss: 3.4477\n",
      "Epoch: 396/1000 | Training Loss: 0.5189 | Validation Loss: 3.4500\n",
      "Epoch: 397/1000 | Training Loss: 0.5184 | Validation Loss: 3.4648\n",
      "Epoch: 398/1000 | Training Loss: 0.5184 | Validation Loss: 3.4616\n",
      "Epoch: 399/1000 | Training Loss: 0.5180 | Validation Loss: 3.4783\n",
      "Epoch: 400/1000 | Training Loss: 0.5177 | Validation Loss: 3.4729\n",
      "Epoch: 401/1000 | Training Loss: 0.5156 | Validation Loss: 3.4845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 402/1000 | Training Loss: 0.5132 | Validation Loss: 3.4851\n",
      "Epoch: 403/1000 | Training Loss: 0.5104 | Validation Loss: 3.4911\n",
      "Epoch: 404/1000 | Training Loss: 0.5087 | Validation Loss: 3.4998\n",
      "Epoch: 405/1000 | Training Loss: 0.5081 | Validation Loss: 3.5023\n",
      "Epoch: 406/1000 | Training Loss: 0.5078 | Validation Loss: 3.5162\n",
      "Epoch: 407/1000 | Training Loss: 0.5076 | Validation Loss: 3.5181\n",
      "Epoch: 408/1000 | Training Loss: 0.5068 | Validation Loss: 3.5287\n",
      "Epoch: 409/1000 | Training Loss: 0.5058 | Validation Loss: 3.5312\n",
      "Epoch: 410/1000 | Training Loss: 0.5041 | Validation Loss: 3.5402\n",
      "Epoch: 411/1000 | Training Loss: 0.5026 | Validation Loss: 3.5430\n",
      "Epoch: 412/1000 | Training Loss: 0.5009 | Validation Loss: 3.5510\n",
      "Epoch: 413/1000 | Training Loss: 0.4994 | Validation Loss: 3.5559\n",
      "Epoch: 414/1000 | Training Loss: 0.4981 | Validation Loss: 3.5635\n",
      "Epoch: 415/1000 | Training Loss: 0.4972 | Validation Loss: 3.5695\n",
      "Epoch: 416/1000 | Training Loss: 0.4967 | Validation Loss: 3.5737\n",
      "Epoch: 417/1000 | Training Loss: 0.4961 | Validation Loss: 3.5847\n",
      "Epoch: 418/1000 | Training Loss: 0.4957 | Validation Loss: 3.5845\n",
      "Epoch: 419/1000 | Training Loss: 0.4950 | Validation Loss: 3.5987\n",
      "Epoch: 420/1000 | Training Loss: 0.4942 | Validation Loss: 3.5952\n",
      "Epoch: 421/1000 | Training Loss: 0.4927 | Validation Loss: 3.6090\n",
      "Epoch: 422/1000 | Training Loss: 0.4912 | Validation Loss: 3.6078\n",
      "Epoch: 423/1000 | Training Loss: 0.4895 | Validation Loss: 3.6164\n",
      "Epoch: 424/1000 | Training Loss: 0.4882 | Validation Loss: 3.6225\n",
      "Epoch: 425/1000 | Training Loss: 0.4870 | Validation Loss: 3.6256\n",
      "Epoch: 426/1000 | Training Loss: 0.4859 | Validation Loss: 3.6352\n",
      "Epoch: 427/1000 | Training Loss: 0.4850 | Validation Loss: 3.6356\n",
      "Epoch: 428/1000 | Training Loss: 0.4844 | Validation Loss: 3.6487\n",
      "Epoch: 429/1000 | Training Loss: 0.4841 | Validation Loss: 3.6505\n",
      "Epoch: 430/1000 | Training Loss: 0.4836 | Validation Loss: 3.6632\n",
      "Epoch: 431/1000 | Training Loss: 0.4834 | Validation Loss: 3.6656\n",
      "Epoch: 432/1000 | Training Loss: 0.4823 | Validation Loss: 3.6750\n",
      "Epoch: 433/1000 | Training Loss: 0.4810 | Validation Loss: 3.6737\n",
      "Epoch: 434/1000 | Training Loss: 0.4789 | Validation Loss: 3.6830\n",
      "Epoch: 435/1000 | Training Loss: 0.4773 | Validation Loss: 3.6849\n",
      "Epoch: 436/1000 | Training Loss: 0.4760 | Validation Loss: 3.6945\n",
      "Epoch: 437/1000 | Training Loss: 0.4751 | Validation Loss: 3.6996\n",
      "Epoch: 438/1000 | Training Loss: 0.4743 | Validation Loss: 3.7072\n",
      "Epoch: 439/1000 | Training Loss: 0.4734 | Validation Loss: 3.7190\n",
      "Epoch: 440/1000 | Training Loss: 0.4727 | Validation Loss: 3.7179\n",
      "Epoch: 441/1000 | Training Loss: 0.4722 | Validation Loss: 3.7331\n",
      "Epoch: 442/1000 | Training Loss: 0.4721 | Validation Loss: 3.7300\n",
      "Epoch: 443/1000 | Training Loss: 0.4718 | Validation Loss: 3.7485\n",
      "Epoch: 444/1000 | Training Loss: 0.4715 | Validation Loss: 3.7443\n",
      "Epoch: 445/1000 | Training Loss: 0.4702 | Validation Loss: 3.7583\n",
      "Epoch: 446/1000 | Training Loss: 0.4686 | Validation Loss: 3.7597\n",
      "Epoch: 447/1000 | Training Loss: 0.4666 | Validation Loss: 3.7674\n",
      "Epoch: 448/1000 | Training Loss: 0.4649 | Validation Loss: 3.7727\n",
      "Epoch: 449/1000 | Training Loss: 0.4636 | Validation Loss: 3.7761\n",
      "Epoch: 450/1000 | Training Loss: 0.4627 | Validation Loss: 3.7860\n",
      "Epoch: 451/1000 | Training Loss: 0.4621 | Validation Loss: 3.7906\n",
      "Epoch: 452/1000 | Training Loss: 0.4617 | Validation Loss: 3.8034\n",
      "Epoch: 453/1000 | Training Loss: 0.4614 | Validation Loss: 3.8055\n",
      "Epoch: 454/1000 | Training Loss: 0.4608 | Validation Loss: 3.8172\n",
      "Epoch: 455/1000 | Training Loss: 0.4604 | Validation Loss: 3.8156\n",
      "Epoch: 456/1000 | Training Loss: 0.4593 | Validation Loss: 3.8309\n",
      "Epoch: 457/1000 | Training Loss: 0.4585 | Validation Loss: 3.8257\n",
      "Epoch: 458/1000 | Training Loss: 0.4572 | Validation Loss: 3.8415\n",
      "Epoch: 459/1000 | Training Loss: 0.4557 | Validation Loss: 3.8387\n",
      "Epoch: 460/1000 | Training Loss: 0.4541 | Validation Loss: 3.8489\n",
      "Epoch: 461/1000 | Training Loss: 0.4526 | Validation Loss: 3.8541\n",
      "Epoch: 462/1000 | Training Loss: 0.4517 | Validation Loss: 3.8563\n",
      "Epoch: 463/1000 | Training Loss: 0.4512 | Validation Loss: 3.8700\n",
      "Epoch: 464/1000 | Training Loss: 0.4512 | Validation Loss: 3.8658\n",
      "Epoch: 465/1000 | Training Loss: 0.4510 | Validation Loss: 3.8818\n",
      "Epoch: 466/1000 | Training Loss: 0.4506 | Validation Loss: 3.8777\n",
      "Epoch: 467/1000 | Training Loss: 0.4495 | Validation Loss: 3.8912\n",
      "Epoch: 468/1000 | Training Loss: 0.4484 | Validation Loss: 3.8897\n",
      "Epoch: 469/1000 | Training Loss: 0.4466 | Validation Loss: 3.8975\n",
      "Epoch: 470/1000 | Training Loss: 0.4452 | Validation Loss: 3.9021\n",
      "Epoch: 471/1000 | Training Loss: 0.4440 | Validation Loss: 3.9067\n",
      "Epoch: 472/1000 | Training Loss: 0.4430 | Validation Loss: 3.9137\n",
      "Epoch: 473/1000 | Training Loss: 0.4421 | Validation Loss: 3.9190\n",
      "Epoch: 474/1000 | Training Loss: 0.4415 | Validation Loss: 3.9267\n",
      "Epoch: 475/1000 | Training Loss: 0.4409 | Validation Loss: 3.9316\n",
      "Epoch: 476/1000 | Training Loss: 0.4405 | Validation Loss: 3.9409\n",
      "Epoch: 477/1000 | Training Loss: 0.4402 | Validation Loss: 3.9402\n",
      "Epoch: 478/1000 | Training Loss: 0.4397 | Validation Loss: 3.9541\n",
      "Epoch: 479/1000 | Training Loss: 0.4395 | Validation Loss: 3.9482\n",
      "Epoch: 480/1000 | Training Loss: 0.4388 | Validation Loss: 3.9679\n",
      "Epoch: 481/1000 | Training Loss: 0.4377 | Validation Loss: 3.9608\n",
      "Epoch: 482/1000 | Training Loss: 0.4360 | Validation Loss: 3.9771\n",
      "Epoch: 483/1000 | Training Loss: 0.4343 | Validation Loss: 3.9732\n",
      "Epoch: 484/1000 | Training Loss: 0.4328 | Validation Loss: 3.9802\n",
      "Epoch: 485/1000 | Training Loss: 0.4320 | Validation Loss: 3.9898\n",
      "Epoch: 486/1000 | Training Loss: 0.4316 | Validation Loss: 3.9892\n",
      "Epoch: 487/1000 | Training Loss: 0.4313 | Validation Loss: 4.0062\n",
      "Epoch: 488/1000 | Training Loss: 0.4308 | Validation Loss: 4.0014\n",
      "Epoch: 489/1000 | Training Loss: 0.4301 | Validation Loss: 4.0160\n",
      "Epoch: 490/1000 | Training Loss: 0.4295 | Validation Loss: 4.0136\n",
      "Epoch: 491/1000 | Training Loss: 0.4290 | Validation Loss: 4.0267\n",
      "Epoch: 492/1000 | Training Loss: 0.4290 | Validation Loss: 4.0295\n",
      "Epoch: 493/1000 | Training Loss: 0.4282 | Validation Loss: 4.0395\n",
      "Epoch: 494/1000 | Training Loss: 0.4271 | Validation Loss: 4.0384\n",
      "Epoch: 495/1000 | Training Loss: 0.4250 | Validation Loss: 4.0461\n",
      "Epoch: 496/1000 | Training Loss: 0.4233 | Validation Loss: 4.0441\n",
      "Epoch: 497/1000 | Training Loss: 0.4223 | Validation Loss: 4.0559\n",
      "Epoch: 498/1000 | Training Loss: 0.4217 | Validation Loss: 4.0589\n",
      "Epoch: 499/1000 | Training Loss: 0.4213 | Validation Loss: 4.0660\n",
      "Epoch: 500/1000 | Training Loss: 0.4206 | Validation Loss: 4.0734\n",
      "Epoch: 501/1000 | Training Loss: 0.4200 | Validation Loss: 4.0684\n",
      "Epoch: 502/1000 | Training Loss: 0.4195 | Validation Loss: 4.0862\n",
      "Epoch: 503/1000 | Training Loss: 0.4195 | Validation Loss: 4.0752\n",
      "Epoch: 504/1000 | Training Loss: 0.4190 | Validation Loss: 4.0993\n",
      "Epoch: 505/1000 | Training Loss: 0.4184 | Validation Loss: 4.0906\n",
      "Epoch: 506/1000 | Training Loss: 0.4172 | Validation Loss: 4.1023\n",
      "Epoch: 507/1000 | Training Loss: 0.4161 | Validation Loss: 4.1021\n",
      "Epoch: 508/1000 | Training Loss: 0.4149 | Validation Loss: 4.1042\n",
      "Epoch: 509/1000 | Training Loss: 0.4138 | Validation Loss: 4.1132\n",
      "Epoch: 510/1000 | Training Loss: 0.4125 | Validation Loss: 4.1108\n",
      "Epoch: 511/1000 | Training Loss: 0.4111 | Validation Loss: 4.1209\n",
      "Epoch: 512/1000 | Training Loss: 0.4102 | Validation Loss: 4.1263\n",
      "Epoch: 513/1000 | Training Loss: 0.4098 | Validation Loss: 4.1319\n",
      "Epoch: 514/1000 | Training Loss: 0.4099 | Validation Loss: 4.1351\n",
      "Epoch: 515/1000 | Training Loss: 0.4100 | Validation Loss: 4.1462\n",
      "Epoch: 516/1000 | Training Loss: 0.4100 | Validation Loss: 4.1450\n",
      "Epoch: 517/1000 | Training Loss: 0.4091 | Validation Loss: 4.1608\n",
      "Epoch: 518/1000 | Training Loss: 0.4083 | Validation Loss: 4.1513\n",
      "Epoch: 519/1000 | Training Loss: 0.4072 | Validation Loss: 4.1724\n",
      "Epoch: 520/1000 | Training Loss: 0.4060 | Validation Loss: 4.1644\n",
      "Epoch: 521/1000 | Training Loss: 0.4045 | Validation Loss: 4.1788\n",
      "Epoch: 522/1000 | Training Loss: 0.4033 | Validation Loss: 4.1783\n",
      "Epoch: 523/1000 | Training Loss: 0.4023 | Validation Loss: 4.1811\n",
      "Epoch: 524/1000 | Training Loss: 0.4017 | Validation Loss: 4.1953\n",
      "Epoch: 525/1000 | Training Loss: 0.4011 | Validation Loss: 4.1938\n",
      "Epoch: 526/1000 | Training Loss: 0.4007 | Validation Loss: 4.2103\n",
      "Epoch: 527/1000 | Training Loss: 0.4004 | Validation Loss: 4.2033\n",
      "Epoch: 528/1000 | Training Loss: 0.4001 | Validation Loss: 4.2178\n",
      "Epoch: 529/1000 | Training Loss: 0.4002 | Validation Loss: 4.2167\n",
      "Epoch: 530/1000 | Training Loss: 0.3998 | Validation Loss: 4.2299\n",
      "Epoch: 531/1000 | Training Loss: 0.3994 | Validation Loss: 4.2258\n",
      "Epoch: 532/1000 | Training Loss: 0.3978 | Validation Loss: 4.2403\n",
      "Epoch: 533/1000 | Training Loss: 0.3964 | Validation Loss: 4.2377\n",
      "Epoch: 534/1000 | Training Loss: 0.3949 | Validation Loss: 4.2478\n",
      "Epoch: 535/1000 | Training Loss: 0.3938 | Validation Loss: 4.2459\n",
      "Epoch: 536/1000 | Training Loss: 0.3931 | Validation Loss: 4.2559\n",
      "Epoch: 537/1000 | Training Loss: 0.3925 | Validation Loss: 4.2681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 538/1000 | Training Loss: 0.3921 | Validation Loss: 4.2688\n",
      "Epoch: 539/1000 | Training Loss: 0.3918 | Validation Loss: 4.2853\n",
      "Epoch: 540/1000 | Training Loss: 0.3918 | Validation Loss: 4.2754\n",
      "Epoch: 541/1000 | Training Loss: 0.3917 | Validation Loss: 4.2957\n",
      "Epoch: 542/1000 | Training Loss: 0.3917 | Validation Loss: 4.2861\n",
      "Epoch: 543/1000 | Training Loss: 0.3911 | Validation Loss: 4.3066\n",
      "Epoch: 544/1000 | Training Loss: 0.3903 | Validation Loss: 4.2997\n",
      "Epoch: 545/1000 | Training Loss: 0.3888 | Validation Loss: 4.3115\n",
      "Epoch: 546/1000 | Training Loss: 0.3874 | Validation Loss: 4.3154\n",
      "Epoch: 547/1000 | Training Loss: 0.3859 | Validation Loss: 4.3160\n",
      "Epoch: 548/1000 | Training Loss: 0.3849 | Validation Loss: 4.3249\n",
      "Epoch: 549/1000 | Training Loss: 0.3841 | Validation Loss: 4.3268\n",
      "Epoch: 550/1000 | Training Loss: 0.3837 | Validation Loss: 4.3399\n",
      "Epoch: 551/1000 | Training Loss: 0.3835 | Validation Loss: 4.3394\n",
      "Epoch: 552/1000 | Training Loss: 0.3836 | Validation Loss: 4.3522\n",
      "Epoch: 553/1000 | Training Loss: 0.3838 | Validation Loss: 4.3516\n",
      "Epoch: 554/1000 | Training Loss: 0.3836 | Validation Loss: 4.3687\n",
      "Epoch: 555/1000 | Training Loss: 0.3833 | Validation Loss: 4.3609\n",
      "Epoch: 556/1000 | Training Loss: 0.3819 | Validation Loss: 4.3766\n",
      "Epoch: 557/1000 | Training Loss: 0.3805 | Validation Loss: 4.3702\n",
      "Epoch: 558/1000 | Training Loss: 0.3789 | Validation Loss: 4.3855\n",
      "Epoch: 559/1000 | Training Loss: 0.3777 | Validation Loss: 4.3831\n",
      "Epoch: 560/1000 | Training Loss: 0.3770 | Validation Loss: 4.3910\n",
      "Epoch: 561/1000 | Training Loss: 0.3765 | Validation Loss: 4.3977\n",
      "Epoch: 562/1000 | Training Loss: 0.3762 | Validation Loss: 4.3972\n",
      "Epoch: 563/1000 | Training Loss: 0.3760 | Validation Loss: 4.4139\n",
      "Epoch: 564/1000 | Training Loss: 0.3760 | Validation Loss: 4.4070\n",
      "Epoch: 565/1000 | Training Loss: 0.3758 | Validation Loss: 4.4302\n",
      "Epoch: 566/1000 | Training Loss: 0.3755 | Validation Loss: 4.4165\n",
      "Epoch: 567/1000 | Training Loss: 0.3743 | Validation Loss: 4.4351\n",
      "Epoch: 568/1000 | Training Loss: 0.3732 | Validation Loss: 4.4301\n",
      "Epoch: 569/1000 | Training Loss: 0.3718 | Validation Loss: 4.4370\n",
      "Epoch: 570/1000 | Training Loss: 0.3709 | Validation Loss: 4.4465\n",
      "Epoch: 571/1000 | Training Loss: 0.3702 | Validation Loss: 4.4473\n",
      "Epoch: 572/1000 | Training Loss: 0.3697 | Validation Loss: 4.4597\n",
      "Epoch: 573/1000 | Training Loss: 0.3692 | Validation Loss: 4.4561\n",
      "Epoch: 574/1000 | Training Loss: 0.3688 | Validation Loss: 4.4707\n",
      "Epoch: 575/1000 | Training Loss: 0.3685 | Validation Loss: 4.4707\n",
      "Epoch: 576/1000 | Training Loss: 0.3682 | Validation Loss: 4.4806\n",
      "Epoch: 577/1000 | Training Loss: 0.3680 | Validation Loss: 4.4799\n",
      "Epoch: 578/1000 | Training Loss: 0.3677 | Validation Loss: 4.4955\n",
      "Epoch: 579/1000 | Training Loss: 0.3674 | Validation Loss: 4.4932\n",
      "Epoch: 580/1000 | Training Loss: 0.3665 | Validation Loss: 4.5082\n",
      "Epoch: 581/1000 | Training Loss: 0.3658 | Validation Loss: 4.4983\n",
      "Epoch: 582/1000 | Training Loss: 0.3648 | Validation Loss: 4.5169\n",
      "Epoch: 583/1000 | Training Loss: 0.3639 | Validation Loss: 4.5079\n",
      "Epoch: 584/1000 | Training Loss: 0.3629 | Validation Loss: 4.5240\n",
      "Epoch: 585/1000 | Training Loss: 0.3617 | Validation Loss: 4.5279\n",
      "Epoch: 586/1000 | Training Loss: 0.3609 | Validation Loss: 4.5266\n",
      "Epoch: 587/1000 | Training Loss: 0.3606 | Validation Loss: 4.5462\n",
      "Epoch: 588/1000 | Training Loss: 0.3608 | Validation Loss: 4.5356\n",
      "Epoch: 589/1000 | Training Loss: 0.3610 | Validation Loss: 4.5615\n",
      "Epoch: 590/1000 | Training Loss: 0.3610 | Validation Loss: 4.5461\n",
      "Epoch: 591/1000 | Training Loss: 0.3601 | Validation Loss: 4.5653\n",
      "Epoch: 592/1000 | Training Loss: 0.3593 | Validation Loss: 4.5608\n",
      "Epoch: 593/1000 | Training Loss: 0.3580 | Validation Loss: 4.5691\n",
      "Epoch: 594/1000 | Training Loss: 0.3570 | Validation Loss: 4.5755\n",
      "Epoch: 595/1000 | Training Loss: 0.3561 | Validation Loss: 4.5775\n",
      "Epoch: 596/1000 | Training Loss: 0.3552 | Validation Loss: 4.5869\n",
      "Epoch: 597/1000 | Training Loss: 0.3545 | Validation Loss: 4.5881\n",
      "Epoch: 598/1000 | Training Loss: 0.3541 | Validation Loss: 4.5982\n",
      "Epoch: 599/1000 | Training Loss: 0.3539 | Validation Loss: 4.6026\n",
      "Epoch: 600/1000 | Training Loss: 0.3537 | Validation Loss: 4.6094\n",
      "Epoch: 601/1000 | Training Loss: 0.3533 | Validation Loss: 4.6118\n",
      "Epoch: 602/1000 | Training Loss: 0.3527 | Validation Loss: 4.6215\n",
      "Epoch: 603/1000 | Training Loss: 0.3522 | Validation Loss: 4.6179\n",
      "Epoch: 604/1000 | Training Loss: 0.3515 | Validation Loss: 4.6391\n",
      "Epoch: 605/1000 | Training Loss: 0.3512 | Validation Loss: 4.6258\n",
      "Epoch: 606/1000 | Training Loss: 0.3510 | Validation Loss: 4.6482\n",
      "Epoch: 607/1000 | Training Loss: 0.3506 | Validation Loss: 4.6378\n",
      "Epoch: 608/1000 | Training Loss: 0.3495 | Validation Loss: 4.6535\n",
      "Epoch: 609/1000 | Training Loss: 0.3481 | Validation Loss: 4.6539\n",
      "Epoch: 610/1000 | Training Loss: 0.3470 | Validation Loss: 4.6578\n",
      "Epoch: 611/1000 | Training Loss: 0.3463 | Validation Loss: 4.6694\n",
      "Epoch: 612/1000 | Training Loss: 0.3459 | Validation Loss: 4.6660\n",
      "Epoch: 613/1000 | Training Loss: 0.3455 | Validation Loss: 4.6765\n",
      "Epoch: 614/1000 | Training Loss: 0.3450 | Validation Loss: 4.6796\n",
      "Epoch: 615/1000 | Training Loss: 0.3445 | Validation Loss: 4.6859\n",
      "Epoch: 616/1000 | Training Loss: 0.3439 | Validation Loss: 4.6923\n",
      "Epoch: 617/1000 | Training Loss: 0.3433 | Validation Loss: 4.6953\n",
      "Epoch: 618/1000 | Training Loss: 0.3428 | Validation Loss: 4.6977\n",
      "Epoch: 619/1000 | Training Loss: 0.3422 | Validation Loss: 4.7039\n",
      "Epoch: 620/1000 | Training Loss: 0.3418 | Validation Loss: 4.7058\n",
      "Epoch: 621/1000 | Training Loss: 0.3418 | Validation Loss: 4.7230\n",
      "Epoch: 622/1000 | Training Loss: 0.3422 | Validation Loss: 4.7121\n",
      "Epoch: 623/1000 | Training Loss: 0.3424 | Validation Loss: 4.7412\n",
      "Epoch: 624/1000 | Training Loss: 0.3428 | Validation Loss: 4.7234\n",
      "Epoch: 625/1000 | Training Loss: 0.3421 | Validation Loss: 4.7488\n",
      "Epoch: 626/1000 | Training Loss: 0.3417 | Validation Loss: 4.7360\n",
      "Epoch: 627/1000 | Training Loss: 0.3404 | Validation Loss: 4.7538\n",
      "Epoch: 628/1000 | Training Loss: 0.3400 | Validation Loss: 4.7495\n",
      "Epoch: 629/1000 | Training Loss: 0.3388 | Validation Loss: 4.7564\n",
      "Epoch: 630/1000 | Training Loss: 0.3374 | Validation Loss: 4.7575\n",
      "Epoch: 631/1000 | Training Loss: 0.3359 | Validation Loss: 4.7670\n",
      "Epoch: 632/1000 | Training Loss: 0.3351 | Validation Loss: 4.7714\n",
      "Epoch: 633/1000 | Training Loss: 0.3350 | Validation Loss: 4.7742\n",
      "Epoch: 634/1000 | Training Loss: 0.3352 | Validation Loss: 4.7860\n",
      "Epoch: 635/1000 | Training Loss: 0.3354 | Validation Loss: 4.7773\n",
      "Epoch: 636/1000 | Training Loss: 0.3353 | Validation Loss: 4.8008\n",
      "Epoch: 637/1000 | Training Loss: 0.3351 | Validation Loss: 4.7804\n",
      "Epoch: 638/1000 | Training Loss: 0.3341 | Validation Loss: 4.8064\n",
      "Epoch: 639/1000 | Training Loss: 0.3330 | Validation Loss: 4.7909\n",
      "Epoch: 640/1000 | Training Loss: 0.3316 | Validation Loss: 4.8037\n",
      "Epoch: 641/1000 | Training Loss: 0.3305 | Validation Loss: 4.8051\n",
      "Epoch: 642/1000 | Training Loss: 0.3299 | Validation Loss: 4.8022\n",
      "Epoch: 643/1000 | Training Loss: 0.3295 | Validation Loss: 4.8235\n",
      "Epoch: 644/1000 | Training Loss: 0.3291 | Validation Loss: 4.8131\n",
      "Epoch: 645/1000 | Training Loss: 0.3285 | Validation Loss: 4.8291\n",
      "Epoch: 646/1000 | Training Loss: 0.3281 | Validation Loss: 4.8238\n",
      "Epoch: 647/1000 | Training Loss: 0.3280 | Validation Loss: 4.8379\n",
      "Epoch: 648/1000 | Training Loss: 0.3284 | Validation Loss: 4.8377\n",
      "Epoch: 649/1000 | Training Loss: 0.3282 | Validation Loss: 4.8489\n",
      "Epoch: 650/1000 | Training Loss: 0.3279 | Validation Loss: 4.8466\n",
      "Epoch: 651/1000 | Training Loss: 0.3267 | Validation Loss: 4.8614\n",
      "Epoch: 652/1000 | Training Loss: 0.3257 | Validation Loss: 4.8478\n",
      "Epoch: 653/1000 | Training Loss: 0.3248 | Validation Loss: 4.8679\n",
      "Epoch: 654/1000 | Training Loss: 0.3240 | Validation Loss: 4.8614\n",
      "Epoch: 655/1000 | Training Loss: 0.3233 | Validation Loss: 4.8738\n",
      "Epoch: 656/1000 | Training Loss: 0.3226 | Validation Loss: 4.8726\n",
      "Epoch: 657/1000 | Training Loss: 0.3221 | Validation Loss: 4.8790\n",
      "Epoch: 658/1000 | Training Loss: 0.3216 | Validation Loss: 4.8891\n",
      "Epoch: 659/1000 | Training Loss: 0.3212 | Validation Loss: 4.8855\n",
      "Epoch: 660/1000 | Training Loss: 0.3209 | Validation Loss: 4.9038\n",
      "Epoch: 661/1000 | Training Loss: 0.3207 | Validation Loss: 4.8965\n",
      "Epoch: 662/1000 | Training Loss: 0.3207 | Validation Loss: 4.9162\n",
      "Epoch: 663/1000 | Training Loss: 0.3210 | Validation Loss: 4.9083\n",
      "Epoch: 664/1000 | Training Loss: 0.3212 | Validation Loss: 4.9263\n",
      "Epoch: 665/1000 | Training Loss: 0.3214 | Validation Loss: 4.9164\n",
      "Epoch: 666/1000 | Training Loss: 0.3205 | Validation Loss: 4.9403\n",
      "Epoch: 667/1000 | Training Loss: 0.3195 | Validation Loss: 4.9241\n",
      "Epoch: 668/1000 | Training Loss: 0.3180 | Validation Loss: 4.9424\n",
      "Epoch: 669/1000 | Training Loss: 0.3167 | Validation Loss: 4.9357\n",
      "Epoch: 670/1000 | Training Loss: 0.3157 | Validation Loss: 4.9476\n",
      "Epoch: 671/1000 | Training Loss: 0.3152 | Validation Loss: 4.9565\n",
      "Epoch: 672/1000 | Training Loss: 0.3151 | Validation Loss: 4.9535\n",
      "Epoch: 673/1000 | Training Loss: 0.3153 | Validation Loss: 4.9776\n",
      "Epoch: 674/1000 | Training Loss: 0.3159 | Validation Loss: 4.9606\n",
      "Epoch: 675/1000 | Training Loss: 0.3162 | Validation Loss: 4.9903\n",
      "Epoch: 676/1000 | Training Loss: 0.3166 | Validation Loss: 4.9731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 677/1000 | Training Loss: 0.3157 | Validation Loss: 4.9968\n",
      "Epoch: 678/1000 | Training Loss: 0.3149 | Validation Loss: 4.9890\n",
      "Epoch: 679/1000 | Training Loss: 0.3130 | Validation Loss: 4.9949\n",
      "Epoch: 680/1000 | Training Loss: 0.3113 | Validation Loss: 4.9992\n",
      "Epoch: 681/1000 | Training Loss: 0.3104 | Validation Loss: 4.9996\n",
      "Epoch: 682/1000 | Training Loss: 0.3101 | Validation Loss: 5.0141\n",
      "Epoch: 683/1000 | Training Loss: 0.3104 | Validation Loss: 5.0101\n",
      "Epoch: 684/1000 | Training Loss: 0.3107 | Validation Loss: 5.0342\n",
      "Epoch: 685/1000 | Training Loss: 0.3112 | Validation Loss: 5.0231\n",
      "Epoch: 686/1000 | Training Loss: 0.3106 | Validation Loss: 5.0439\n",
      "Epoch: 687/1000 | Training Loss: 0.3101 | Validation Loss: 5.0335\n",
      "Epoch: 688/1000 | Training Loss: 0.3086 | Validation Loss: 5.0473\n",
      "Epoch: 689/1000 | Training Loss: 0.3073 | Validation Loss: 5.0403\n",
      "Epoch: 690/1000 | Training Loss: 0.3063 | Validation Loss: 5.0543\n",
      "Epoch: 691/1000 | Training Loss: 0.3058 | Validation Loss: 5.0615\n",
      "Epoch: 692/1000 | Training Loss: 0.3057 | Validation Loss: 5.0631\n",
      "Epoch: 693/1000 | Training Loss: 0.3059 | Validation Loss: 5.0763\n",
      "Epoch: 694/1000 | Training Loss: 0.3061 | Validation Loss: 5.0717\n",
      "Epoch: 695/1000 | Training Loss: 0.3060 | Validation Loss: 5.0913\n",
      "Epoch: 696/1000 | Training Loss: 0.3060 | Validation Loss: 5.0777\n",
      "Epoch: 697/1000 | Training Loss: 0.3052 | Validation Loss: 5.1029\n",
      "Epoch: 698/1000 | Training Loss: 0.3045 | Validation Loss: 5.0903\n",
      "Epoch: 699/1000 | Training Loss: 0.3033 | Validation Loss: 5.1062\n",
      "Epoch: 700/1000 | Training Loss: 0.3023 | Validation Loss: 5.1041\n",
      "Epoch: 701/1000 | Training Loss: 0.3017 | Validation Loss: 5.1137\n",
      "Epoch: 702/1000 | Training Loss: 0.3014 | Validation Loss: 5.1230\n",
      "Epoch: 703/1000 | Training Loss: 0.3012 | Validation Loss: 5.1235\n",
      "Epoch: 704/1000 | Training Loss: 0.3011 | Validation Loss: 5.1384\n",
      "Epoch: 705/1000 | Training Loss: 0.3011 | Validation Loss: 5.1314\n",
      "Epoch: 706/1000 | Training Loss: 0.3007 | Validation Loss: 5.1461\n",
      "Epoch: 707/1000 | Training Loss: 0.3003 | Validation Loss: 5.1388\n",
      "Epoch: 708/1000 | Training Loss: 0.2995 | Validation Loss: 5.1524\n",
      "Epoch: 709/1000 | Training Loss: 0.2990 | Validation Loss: 5.1454\n",
      "Epoch: 710/1000 | Training Loss: 0.2985 | Validation Loss: 5.1621\n",
      "Epoch: 711/1000 | Training Loss: 0.2982 | Validation Loss: 5.1598\n",
      "Epoch: 712/1000 | Training Loss: 0.2980 | Validation Loss: 5.1706\n",
      "Epoch: 713/1000 | Training Loss: 0.2977 | Validation Loss: 5.1636\n",
      "Epoch: 714/1000 | Training Loss: 0.2971 | Validation Loss: 5.1784\n",
      "Epoch: 715/1000 | Training Loss: 0.2963 | Validation Loss: 5.1751\n",
      "Epoch: 716/1000 | Training Loss: 0.2954 | Validation Loss: 5.1888\n",
      "Epoch: 717/1000 | Training Loss: 0.2946 | Validation Loss: 5.1915\n",
      "Epoch: 718/1000 | Training Loss: 0.2941 | Validation Loss: 5.1972\n",
      "Epoch: 719/1000 | Training Loss: 0.2939 | Validation Loss: 5.2063\n",
      "Epoch: 720/1000 | Training Loss: 0.2940 | Validation Loss: 5.2051\n",
      "Epoch: 721/1000 | Training Loss: 0.2941 | Validation Loss: 5.2230\n",
      "Epoch: 722/1000 | Training Loss: 0.2941 | Validation Loss: 5.2130\n",
      "Epoch: 723/1000 | Training Loss: 0.2940 | Validation Loss: 5.2377\n",
      "Epoch: 724/1000 | Training Loss: 0.2937 | Validation Loss: 5.2163\n",
      "Epoch: 725/1000 | Training Loss: 0.2933 | Validation Loss: 5.2453\n",
      "Epoch: 726/1000 | Training Loss: 0.2931 | Validation Loss: 5.2288\n",
      "Epoch: 727/1000 | Training Loss: 0.2927 | Validation Loss: 5.2526\n",
      "Epoch: 728/1000 | Training Loss: 0.2926 | Validation Loss: 5.2502\n",
      "Epoch: 729/1000 | Training Loss: 0.2919 | Validation Loss: 5.2558\n",
      "Epoch: 730/1000 | Training Loss: 0.2912 | Validation Loss: 5.2634\n",
      "Epoch: 731/1000 | Training Loss: 0.2899 | Validation Loss: 5.2621\n",
      "Epoch: 732/1000 | Training Loss: 0.2888 | Validation Loss: 5.2757\n",
      "Epoch: 733/1000 | Training Loss: 0.2880 | Validation Loss: 5.2724\n",
      "Epoch: 734/1000 | Training Loss: 0.2878 | Validation Loss: 5.2849\n",
      "Epoch: 735/1000 | Training Loss: 0.2881 | Validation Loss: 5.2836\n",
      "Epoch: 736/1000 | Training Loss: 0.2886 | Validation Loss: 5.2936\n",
      "Epoch: 737/1000 | Training Loss: 0.2894 | Validation Loss: 5.2921\n",
      "Epoch: 738/1000 | Training Loss: 0.2891 | Validation Loss: 5.3077\n",
      "Epoch: 739/1000 | Training Loss: 0.2888 | Validation Loss: 5.3017\n",
      "Epoch: 740/1000 | Training Loss: 0.2878 | Validation Loss: 5.3232\n",
      "Epoch: 741/1000 | Training Loss: 0.2873 | Validation Loss: 5.3063\n",
      "Epoch: 742/1000 | Training Loss: 0.2868 | Validation Loss: 5.3328\n",
      "Epoch: 743/1000 | Training Loss: 0.2861 | Validation Loss: 5.3193\n",
      "Epoch: 744/1000 | Training Loss: 0.2853 | Validation Loss: 5.3325\n",
      "Epoch: 745/1000 | Training Loss: 0.2841 | Validation Loss: 5.3336\n",
      "Epoch: 746/1000 | Training Loss: 0.2832 | Validation Loss: 5.3341\n",
      "Epoch: 747/1000 | Training Loss: 0.2832 | Validation Loss: 5.3530\n",
      "Epoch: 748/1000 | Training Loss: 0.2838 | Validation Loss: 5.3392\n",
      "Epoch: 749/1000 | Training Loss: 0.2842 | Validation Loss: 5.3701\n",
      "Epoch: 750/1000 | Training Loss: 0.2843 | Validation Loss: 5.3514\n",
      "Epoch: 751/1000 | Training Loss: 0.2836 | Validation Loss: 5.3742\n",
      "Epoch: 752/1000 | Training Loss: 0.2829 | Validation Loss: 5.3638\n",
      "Epoch: 753/1000 | Training Loss: 0.2818 | Validation Loss: 5.3777\n",
      "Epoch: 754/1000 | Training Loss: 0.2811 | Validation Loss: 5.3808\n",
      "Epoch: 755/1000 | Training Loss: 0.2804 | Validation Loss: 5.3782\n",
      "Epoch: 756/1000 | Training Loss: 0.2798 | Validation Loss: 5.3892\n",
      "Epoch: 757/1000 | Training Loss: 0.2791 | Validation Loss: 5.3902\n",
      "Epoch: 758/1000 | Training Loss: 0.2785 | Validation Loss: 5.3969\n",
      "Epoch: 759/1000 | Training Loss: 0.2783 | Validation Loss: 5.3983\n",
      "Epoch: 760/1000 | Training Loss: 0.2783 | Validation Loss: 5.4130\n",
      "Epoch: 761/1000 | Training Loss: 0.2787 | Validation Loss: 5.4078\n",
      "Epoch: 762/1000 | Training Loss: 0.2788 | Validation Loss: 5.4297\n",
      "Epoch: 763/1000 | Training Loss: 0.2789 | Validation Loss: 5.4170\n",
      "Epoch: 764/1000 | Training Loss: 0.2783 | Validation Loss: 5.4343\n",
      "Epoch: 765/1000 | Training Loss: 0.2778 | Validation Loss: 5.4204\n",
      "Epoch: 766/1000 | Training Loss: 0.2768 | Validation Loss: 5.4406\n",
      "Epoch: 767/1000 | Training Loss: 0.2760 | Validation Loss: 5.4361\n",
      "Epoch: 768/1000 | Training Loss: 0.2752 | Validation Loss: 5.4513\n",
      "Epoch: 769/1000 | Training Loss: 0.2746 | Validation Loss: 5.4497\n",
      "Epoch: 770/1000 | Training Loss: 0.2740 | Validation Loss: 5.4540\n",
      "Epoch: 771/1000 | Training Loss: 0.2735 | Validation Loss: 5.4635\n",
      "Epoch: 772/1000 | Training Loss: 0.2733 | Validation Loss: 5.4608\n",
      "Epoch: 773/1000 | Training Loss: 0.2734 | Validation Loss: 5.4839\n",
      "Epoch: 774/1000 | Training Loss: 0.2737 | Validation Loss: 5.4728\n",
      "Epoch: 775/1000 | Training Loss: 0.2737 | Validation Loss: 5.4924\n",
      "Epoch: 776/1000 | Training Loss: 0.2737 | Validation Loss: 5.4800\n",
      "Epoch: 777/1000 | Training Loss: 0.2731 | Validation Loss: 5.4962\n",
      "Epoch: 778/1000 | Training Loss: 0.2725 | Validation Loss: 5.4921\n",
      "Epoch: 779/1000 | Training Loss: 0.2716 | Validation Loss: 5.5088\n",
      "Epoch: 780/1000 | Training Loss: 0.2710 | Validation Loss: 5.5077\n",
      "Epoch: 781/1000 | Training Loss: 0.2704 | Validation Loss: 5.5120\n",
      "Epoch: 782/1000 | Training Loss: 0.2698 | Validation Loss: 5.5182\n",
      "Epoch: 783/1000 | Training Loss: 0.2692 | Validation Loss: 5.5195\n",
      "Epoch: 784/1000 | Training Loss: 0.2687 | Validation Loss: 5.5263\n",
      "Epoch: 785/1000 | Training Loss: 0.2682 | Validation Loss: 5.5280\n",
      "Epoch: 786/1000 | Training Loss: 0.2679 | Validation Loss: 5.5426\n",
      "Epoch: 787/1000 | Training Loss: 0.2678 | Validation Loss: 5.5440\n",
      "Epoch: 788/1000 | Training Loss: 0.2679 | Validation Loss: 5.5528\n",
      "Epoch: 789/1000 | Training Loss: 0.2683 | Validation Loss: 5.5503\n",
      "Epoch: 790/1000 | Training Loss: 0.2686 | Validation Loss: 5.5655\n",
      "Epoch: 791/1000 | Training Loss: 0.2691 | Validation Loss: 5.5560\n",
      "Epoch: 792/1000 | Training Loss: 0.2688 | Validation Loss: 5.5810\n",
      "Epoch: 793/1000 | Training Loss: 0.2686 | Validation Loss: 5.5667\n",
      "Epoch: 794/1000 | Training Loss: 0.2675 | Validation Loss: 5.5917\n",
      "Epoch: 795/1000 | Training Loss: 0.2665 | Validation Loss: 5.5735\n",
      "Epoch: 796/1000 | Training Loss: 0.2653 | Validation Loss: 5.5944\n",
      "Epoch: 797/1000 | Training Loss: 0.2642 | Validation Loss: 5.5880\n",
      "Epoch: 798/1000 | Training Loss: 0.2637 | Validation Loss: 5.6042\n",
      "Epoch: 799/1000 | Training Loss: 0.2635 | Validation Loss: 5.6123\n",
      "Epoch: 800/1000 | Training Loss: 0.2637 | Validation Loss: 5.6012\n",
      "Epoch: 801/1000 | Training Loss: 0.2641 | Validation Loss: 5.6251\n",
      "Epoch: 802/1000 | Training Loss: 0.2646 | Validation Loss: 5.6061\n",
      "Epoch: 803/1000 | Training Loss: 0.2646 | Validation Loss: 5.6398\n",
      "Epoch: 804/1000 | Training Loss: 0.2645 | Validation Loss: 5.6184\n",
      "Epoch: 805/1000 | Training Loss: 0.2635 | Validation Loss: 5.6432\n",
      "Epoch: 806/1000 | Training Loss: 0.2623 | Validation Loss: 5.6316\n",
      "Epoch: 807/1000 | Training Loss: 0.2610 | Validation Loss: 5.6374\n",
      "Epoch: 808/1000 | Training Loss: 0.2602 | Validation Loss: 5.6480\n",
      "Epoch: 809/1000 | Training Loss: 0.2598 | Validation Loss: 5.6479\n",
      "Epoch: 810/1000 | Training Loss: 0.2599 | Validation Loss: 5.6734\n",
      "Epoch: 811/1000 | Training Loss: 0.2601 | Validation Loss: 5.6521\n",
      "Epoch: 812/1000 | Training Loss: 0.2601 | Validation Loss: 5.6794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 813/1000 | Training Loss: 0.2602 | Validation Loss: 5.6644\n",
      "Epoch: 814/1000 | Training Loss: 0.2601 | Validation Loss: 5.6837\n",
      "Epoch: 815/1000 | Training Loss: 0.2602 | Validation Loss: 5.6807\n",
      "Epoch: 816/1000 | Training Loss: 0.2598 | Validation Loss: 5.6909\n",
      "Epoch: 817/1000 | Training Loss: 0.2594 | Validation Loss: 5.6911\n",
      "Epoch: 818/1000 | Training Loss: 0.2581 | Validation Loss: 5.6867\n",
      "Epoch: 819/1000 | Training Loss: 0.2570 | Validation Loss: 5.6990\n",
      "Epoch: 820/1000 | Training Loss: 0.2561 | Validation Loss: 5.7026\n",
      "Epoch: 821/1000 | Training Loss: 0.2557 | Validation Loss: 5.7109\n",
      "Epoch: 822/1000 | Training Loss: 0.2557 | Validation Loss: 5.7142\n",
      "Epoch: 823/1000 | Training Loss: 0.2559 | Validation Loss: 5.7254\n",
      "Epoch: 824/1000 | Training Loss: 0.2563 | Validation Loss: 5.7230\n",
      "Epoch: 825/1000 | Training Loss: 0.2564 | Validation Loss: 5.7386\n",
      "Epoch: 826/1000 | Training Loss: 0.2567 | Validation Loss: 5.7315\n",
      "Epoch: 827/1000 | Training Loss: 0.2560 | Validation Loss: 5.7560\n",
      "Epoch: 828/1000 | Training Loss: 0.2556 | Validation Loss: 5.7371\n",
      "Epoch: 829/1000 | Training Loss: 0.2547 | Validation Loss: 5.7621\n",
      "Epoch: 830/1000 | Training Loss: 0.2538 | Validation Loss: 5.7475\n",
      "Epoch: 831/1000 | Training Loss: 0.2529 | Validation Loss: 5.7622\n",
      "Epoch: 832/1000 | Training Loss: 0.2523 | Validation Loss: 5.7673\n",
      "Epoch: 833/1000 | Training Loss: 0.2522 | Validation Loss: 5.7675\n",
      "Epoch: 834/1000 | Training Loss: 0.2522 | Validation Loss: 5.7818\n",
      "Epoch: 835/1000 | Training Loss: 0.2524 | Validation Loss: 5.7712\n",
      "Epoch: 836/1000 | Training Loss: 0.2525 | Validation Loss: 5.7898\n",
      "Epoch: 837/1000 | Training Loss: 0.2527 | Validation Loss: 5.7766\n",
      "Epoch: 838/1000 | Training Loss: 0.2525 | Validation Loss: 5.8009\n",
      "Epoch: 839/1000 | Training Loss: 0.2524 | Validation Loss: 5.7898\n",
      "Epoch: 840/1000 | Training Loss: 0.2515 | Validation Loss: 5.8066\n",
      "Epoch: 841/1000 | Training Loss: 0.2507 | Validation Loss: 5.8024\n",
      "Epoch: 842/1000 | Training Loss: 0.2499 | Validation Loss: 5.8115\n",
      "Epoch: 843/1000 | Training Loss: 0.2491 | Validation Loss: 5.8157\n",
      "Epoch: 844/1000 | Training Loss: 0.2487 | Validation Loss: 5.8173\n",
      "Epoch: 845/1000 | Training Loss: 0.2483 | Validation Loss: 5.8312\n",
      "Epoch: 846/1000 | Training Loss: 0.2482 | Validation Loss: 5.8227\n",
      "Epoch: 847/1000 | Training Loss: 0.2483 | Validation Loss: 5.8393\n",
      "Epoch: 848/1000 | Training Loss: 0.2486 | Validation Loss: 5.8379\n",
      "Epoch: 849/1000 | Training Loss: 0.2489 | Validation Loss: 5.8539\n",
      "Epoch: 850/1000 | Training Loss: 0.2490 | Validation Loss: 5.8436\n",
      "Epoch: 851/1000 | Training Loss: 0.2485 | Validation Loss: 5.8622\n",
      "Epoch: 852/1000 | Training Loss: 0.2477 | Validation Loss: 5.8501\n",
      "Epoch: 853/1000 | Training Loss: 0.2466 | Validation Loss: 5.8687\n",
      "Epoch: 854/1000 | Training Loss: 0.2456 | Validation Loss: 5.8615\n",
      "Epoch: 855/1000 | Training Loss: 0.2450 | Validation Loss: 5.8786\n",
      "Epoch: 856/1000 | Training Loss: 0.2448 | Validation Loss: 5.8811\n",
      "Epoch: 857/1000 | Training Loss: 0.2446 | Validation Loss: 5.8829\n",
      "Epoch: 858/1000 | Training Loss: 0.2447 | Validation Loss: 5.8899\n",
      "Epoch: 859/1000 | Training Loss: 0.2448 | Validation Loss: 5.8889\n",
      "Epoch: 860/1000 | Training Loss: 0.2449 | Validation Loss: 5.9088\n",
      "Epoch: 861/1000 | Training Loss: 0.2450 | Validation Loss: 5.8918\n",
      "Epoch: 862/1000 | Training Loss: 0.2449 | Validation Loss: 5.9191\n",
      "Epoch: 863/1000 | Training Loss: 0.2448 | Validation Loss: 5.8995\n",
      "Epoch: 864/1000 | Training Loss: 0.2442 | Validation Loss: 5.9223\n",
      "Epoch: 865/1000 | Training Loss: 0.2434 | Validation Loss: 5.9136\n",
      "Epoch: 866/1000 | Training Loss: 0.2424 | Validation Loss: 5.9250\n",
      "Epoch: 867/1000 | Training Loss: 0.2418 | Validation Loss: 5.9309\n",
      "Epoch: 868/1000 | Training Loss: 0.2412 | Validation Loss: 5.9267\n",
      "Epoch: 869/1000 | Training Loss: 0.2409 | Validation Loss: 5.9423\n",
      "Epoch: 870/1000 | Training Loss: 0.2407 | Validation Loss: 5.9344\n",
      "Epoch: 871/1000 | Training Loss: 0.2407 | Validation Loss: 5.9580\n",
      "Epoch: 872/1000 | Training Loss: 0.2406 | Validation Loss: 5.9438\n",
      "Epoch: 873/1000 | Training Loss: 0.2405 | Validation Loss: 5.9706\n",
      "Epoch: 874/1000 | Training Loss: 0.2404 | Validation Loss: 5.9533\n",
      "Epoch: 875/1000 | Training Loss: 0.2403 | Validation Loss: 5.9749\n",
      "Epoch: 876/1000 | Training Loss: 0.2401 | Validation Loss: 5.9686\n",
      "Epoch: 877/1000 | Training Loss: 0.2400 | Validation Loss: 5.9769\n",
      "Epoch: 878/1000 | Training Loss: 0.2400 | Validation Loss: 5.9780\n",
      "Epoch: 879/1000 | Training Loss: 0.2398 | Validation Loss: 5.9824\n",
      "Epoch: 880/1000 | Training Loss: 0.2395 | Validation Loss: 5.9855\n",
      "Epoch: 881/1000 | Training Loss: 0.2386 | Validation Loss: 5.9923\n",
      "Epoch: 882/1000 | Training Loss: 0.2378 | Validation Loss: 5.9968\n",
      "Epoch: 883/1000 | Training Loss: 0.2371 | Validation Loss: 6.0019\n",
      "Epoch: 884/1000 | Training Loss: 0.2366 | Validation Loss: 5.9989\n",
      "Epoch: 885/1000 | Training Loss: 0.2364 | Validation Loss: 6.0104\n",
      "Epoch: 886/1000 | Training Loss: 0.2363 | Validation Loss: 6.0114\n",
      "Epoch: 887/1000 | Training Loss: 0.2362 | Validation Loss: 6.0222\n",
      "Epoch: 888/1000 | Training Loss: 0.2361 | Validation Loss: 6.0305\n",
      "Epoch: 889/1000 | Training Loss: 0.2358 | Validation Loss: 6.0342\n",
      "Epoch: 890/1000 | Training Loss: 0.2357 | Validation Loss: 6.0464\n",
      "Epoch: 891/1000 | Training Loss: 0.2359 | Validation Loss: 6.0309\n",
      "Epoch: 892/1000 | Training Loss: 0.2362 | Validation Loss: 6.0600\n",
      "Epoch: 893/1000 | Training Loss: 0.2368 | Validation Loss: 6.0400\n",
      "Epoch: 894/1000 | Training Loss: 0.2366 | Validation Loss: 6.0648\n",
      "Epoch: 895/1000 | Training Loss: 0.2364 | Validation Loss: 6.0540\n",
      "Epoch: 896/1000 | Training Loss: 0.2352 | Validation Loss: 6.0689\n",
      "Epoch: 897/1000 | Training Loss: 0.2342 | Validation Loss: 6.0660\n",
      "Epoch: 898/1000 | Training Loss: 0.2333 | Validation Loss: 6.0700\n",
      "Epoch: 899/1000 | Training Loss: 0.2327 | Validation Loss: 6.0855\n",
      "Epoch: 900/1000 | Training Loss: 0.2323 | Validation Loss: 6.0781\n",
      "Epoch: 901/1000 | Training Loss: 0.2321 | Validation Loss: 6.0942\n",
      "Epoch: 902/1000 | Training Loss: 0.2319 | Validation Loss: 6.0893\n",
      "Epoch: 903/1000 | Training Loss: 0.2318 | Validation Loss: 6.1067\n",
      "Epoch: 904/1000 | Training Loss: 0.2318 | Validation Loss: 6.1033\n",
      "Epoch: 905/1000 | Training Loss: 0.2319 | Validation Loss: 6.1147\n",
      "Epoch: 906/1000 | Training Loss: 0.2324 | Validation Loss: 6.1151\n",
      "Epoch: 907/1000 | Training Loss: 0.2323 | Validation Loss: 6.1231\n",
      "Epoch: 908/1000 | Training Loss: 0.2323 | Validation Loss: 6.1174\n",
      "Epoch: 909/1000 | Training Loss: 0.2319 | Validation Loss: 6.1372\n",
      "Epoch: 910/1000 | Training Loss: 0.2315 | Validation Loss: 6.1181\n",
      "Epoch: 911/1000 | Training Loss: 0.2310 | Validation Loss: 6.1486\n",
      "Epoch: 912/1000 | Training Loss: 0.2306 | Validation Loss: 6.1257\n",
      "Epoch: 913/1000 | Training Loss: 0.2301 | Validation Loss: 6.1568\n",
      "Epoch: 914/1000 | Training Loss: 0.2294 | Validation Loss: 6.1432\n",
      "Epoch: 915/1000 | Training Loss: 0.2288 | Validation Loss: 6.1592\n",
      "Epoch: 916/1000 | Training Loss: 0.2282 | Validation Loss: 6.1629\n",
      "Epoch: 917/1000 | Training Loss: 0.2279 | Validation Loss: 6.1529\n",
      "Epoch: 918/1000 | Training Loss: 0.2280 | Validation Loss: 6.1747\n",
      "Epoch: 919/1000 | Training Loss: 0.2285 | Validation Loss: 6.1616\n",
      "Epoch: 920/1000 | Training Loss: 0.2288 | Validation Loss: 6.1926\n",
      "Epoch: 921/1000 | Training Loss: 0.2291 | Validation Loss: 6.1742\n",
      "Epoch: 922/1000 | Training Loss: 0.2289 | Validation Loss: 6.2016\n",
      "Epoch: 923/1000 | Training Loss: 0.2287 | Validation Loss: 6.1854\n",
      "Epoch: 924/1000 | Training Loss: 0.2279 | Validation Loss: 6.1977\n",
      "Epoch: 925/1000 | Training Loss: 0.2270 | Validation Loss: 6.1956\n",
      "Epoch: 926/1000 | Training Loss: 0.2260 | Validation Loss: 6.2037\n",
      "Epoch: 927/1000 | Training Loss: 0.2251 | Validation Loss: 6.2094\n",
      "Epoch: 928/1000 | Training Loss: 0.2247 | Validation Loss: 6.2091\n",
      "Epoch: 929/1000 | Training Loss: 0.2246 | Validation Loss: 6.2188\n",
      "Epoch: 930/1000 | Training Loss: 0.2247 | Validation Loss: 6.2117\n",
      "Epoch: 931/1000 | Training Loss: 0.2251 | Validation Loss: 6.2320\n",
      "Epoch: 932/1000 | Training Loss: 0.2254 | Validation Loss: 6.2242\n",
      "Epoch: 933/1000 | Training Loss: 0.2256 | Validation Loss: 6.2438\n",
      "Epoch: 934/1000 | Training Loss: 0.2258 | Validation Loss: 6.2281\n",
      "Epoch: 935/1000 | Training Loss: 0.2255 | Validation Loss: 6.2514\n",
      "Epoch: 936/1000 | Training Loss: 0.2253 | Validation Loss: 6.2402\n",
      "Epoch: 937/1000 | Training Loss: 0.2245 | Validation Loss: 6.2543\n",
      "Epoch: 938/1000 | Training Loss: 0.2239 | Validation Loss: 6.2563\n",
      "Epoch: 939/1000 | Training Loss: 0.2230 | Validation Loss: 6.2591\n",
      "Epoch: 940/1000 | Training Loss: 0.2224 | Validation Loss: 6.2688\n",
      "Epoch: 941/1000 | Training Loss: 0.2220 | Validation Loss: 6.2671\n",
      "Epoch: 942/1000 | Training Loss: 0.2219 | Validation Loss: 6.2843\n",
      "Epoch: 943/1000 | Training Loss: 0.2218 | Validation Loss: 6.2696\n",
      "Epoch: 944/1000 | Training Loss: 0.2218 | Validation Loss: 6.2961\n",
      "Epoch: 945/1000 | Training Loss: 0.2219 | Validation Loss: 6.2779\n",
      "Epoch: 946/1000 | Training Loss: 0.2220 | Validation Loss: 6.3048\n",
      "Epoch: 947/1000 | Training Loss: 0.2221 | Validation Loss: 6.2871\n",
      "Epoch: 948/1000 | Training Loss: 0.2216 | Validation Loss: 6.3046\n",
      "Epoch: 949/1000 | Training Loss: 0.2214 | Validation Loss: 6.3024\n",
      "Epoch: 950/1000 | Training Loss: 0.2206 | Validation Loss: 6.3065\n",
      "Epoch: 951/1000 | Training Loss: 0.2201 | Validation Loss: 6.3172\n",
      "Epoch: 952/1000 | Training Loss: 0.2196 | Validation Loss: 6.3082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 953/1000 | Training Loss: 0.2190 | Validation Loss: 6.3317\n",
      "Epoch: 954/1000 | Training Loss: 0.2185 | Validation Loss: 6.3215\n",
      "Epoch: 955/1000 | Training Loss: 0.2183 | Validation Loss: 6.3383\n",
      "Epoch: 956/1000 | Training Loss: 0.2184 | Validation Loss: 6.3301\n",
      "Epoch: 957/1000 | Training Loss: 0.2187 | Validation Loss: 6.3444\n",
      "Epoch: 958/1000 | Training Loss: 0.2193 | Validation Loss: 6.3495\n",
      "Epoch: 959/1000 | Training Loss: 0.2196 | Validation Loss: 6.3580\n",
      "Epoch: 960/1000 | Training Loss: 0.2198 | Validation Loss: 6.3568\n",
      "Epoch: 961/1000 | Training Loss: 0.2189 | Validation Loss: 6.3638\n",
      "Epoch: 962/1000 | Training Loss: 0.2179 | Validation Loss: 6.3549\n",
      "Epoch: 963/1000 | Training Loss: 0.2168 | Validation Loss: 6.3699\n",
      "Epoch: 964/1000 | Training Loss: 0.2161 | Validation Loss: 6.3615\n",
      "Epoch: 965/1000 | Training Loss: 0.2159 | Validation Loss: 6.3832\n",
      "Epoch: 966/1000 | Training Loss: 0.2160 | Validation Loss: 6.3718\n",
      "Epoch: 967/1000 | Training Loss: 0.2162 | Validation Loss: 6.3871\n",
      "Epoch: 968/1000 | Training Loss: 0.2162 | Validation Loss: 6.3864\n",
      "Epoch: 969/1000 | Training Loss: 0.2160 | Validation Loss: 6.3940\n",
      "Epoch: 970/1000 | Training Loss: 0.2156 | Validation Loss: 6.4017\n",
      "Epoch: 971/1000 | Training Loss: 0.2152 | Validation Loss: 6.3951\n",
      "Epoch: 972/1000 | Training Loss: 0.2148 | Validation Loss: 6.4151\n",
      "Epoch: 973/1000 | Training Loss: 0.2145 | Validation Loss: 6.3973\n",
      "Epoch: 974/1000 | Training Loss: 0.2143 | Validation Loss: 6.4213\n",
      "Epoch: 975/1000 | Training Loss: 0.2141 | Validation Loss: 6.4049\n",
      "Epoch: 976/1000 | Training Loss: 0.2142 | Validation Loss: 6.4293\n",
      "Epoch: 977/1000 | Training Loss: 0.2142 | Validation Loss: 6.4205\n",
      "Epoch: 978/1000 | Training Loss: 0.2143 | Validation Loss: 6.4357\n",
      "Epoch: 979/1000 | Training Loss: 0.2140 | Validation Loss: 6.4253\n",
      "Epoch: 980/1000 | Training Loss: 0.2136 | Validation Loss: 6.4405\n",
      "Epoch: 981/1000 | Training Loss: 0.2130 | Validation Loss: 6.4362\n",
      "Epoch: 982/1000 | Training Loss: 0.2125 | Validation Loss: 6.4425\n",
      "Epoch: 983/1000 | Training Loss: 0.2124 | Validation Loss: 6.4446\n",
      "Epoch: 984/1000 | Training Loss: 0.2125 | Validation Loss: 6.4544\n",
      "Epoch: 985/1000 | Training Loss: 0.2127 | Validation Loss: 6.4457\n",
      "Epoch: 986/1000 | Training Loss: 0.2127 | Validation Loss: 6.4622\n",
      "Epoch: 987/1000 | Training Loss: 0.2125 | Validation Loss: 6.4560\n",
      "Epoch: 988/1000 | Training Loss: 0.2119 | Validation Loss: 6.4656\n",
      "Epoch: 989/1000 | Training Loss: 0.2112 | Validation Loss: 6.4653\n",
      "Epoch: 990/1000 | Training Loss: 0.2107 | Validation Loss: 6.4769\n",
      "Epoch: 991/1000 | Training Loss: 0.2105 | Validation Loss: 6.4745\n",
      "Epoch: 992/1000 | Training Loss: 0.2104 | Validation Loss: 6.4880\n",
      "Epoch: 993/1000 | Training Loss: 0.2102 | Validation Loss: 6.4809\n",
      "Epoch: 994/1000 | Training Loss: 0.2100 | Validation Loss: 6.4970\n",
      "Epoch: 995/1000 | Training Loss: 0.2096 | Validation Loss: 6.4849\n",
      "Epoch: 996/1000 | Training Loss: 0.2092 | Validation Loss: 6.5086\n",
      "Epoch: 997/1000 | Training Loss: 0.2088 | Validation Loss: 6.4965\n",
      "Epoch: 998/1000 | Training Loss: 0.2085 | Validation Loss: 6.5135\n",
      "Epoch: 999/1000 | Training Loss: 0.2082 | Validation Loss: 6.5065\n",
      "Epoch: 1000/1000 | Training Loss: 0.2079 | Validation Loss: 6.5141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    loss = train_model(model, optimizer, criterion, X_train, Y_train)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, Y_val).item()\n",
    "        if val_loss < best_val_loss:\n",
    "            best_model = model\n",
    "            best_val_loss = val_loss\n",
    "    print(f'Epoch: {epoch+1}/1000 | Training Loss: {loss:.4f} | Validation Loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9562649726867676"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': best_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, \"./sgg_ckpt_wo_bert.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
