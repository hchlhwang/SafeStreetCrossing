{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6a6HUgxqbGZ",
        "outputId": "5212d6f9-12f2-4036-8523-f6e2d4fc9b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TensorRT-LLM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b v0.8.0 https://github.com/NVIDIA/TensorRT-LLM.git\n",
        "!cd TensorRT-LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lndMC2Rsg26o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjK-tf1NtXdI",
        "outputId": "88d0ae70-7c3e-44e6-85b5-5e32495ab1d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmZT-jeWrAUX",
        "outputId": "95081adf-7632-4b91-ceab-e1a5decc27af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "4pxMblmdrd3q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "#!git remote set-url origin https://{deepika1703}:{12345678}@github.com/{deepika1703}/project.git\n",
        "!git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEg6plOHuZfk",
        "outputId": "e81c91ea-b506-4059-ae52-32bfeb7fcfff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'Meta-Llama-3-8B-Instruct'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 63 (delta 21), reused 11 (delta 11), pack-reused 31 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (63/63), 4.50 MiB | 5.60 MiB/s, done.\n",
            "Filtering content: 100% (6/6), 5.91 GiB | 14.44 MiB/s, done.\n",
            "Encountered 4 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel-00003-of-00004.safetensors\n",
            "\tmodel-00001-of-00004.safetensors\n",
            "\tmodel-00002-of-00004.safetensors\n",
            "\toriginal/consolidated.00.pth\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Drengskapur\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "# @title {display-mode:\"form\"}\n",
        "# @markdown <br/><br/><center><img src=\"https://cdn.jsdelivr.net/gh/drengskapur/docker-in-colab/assets/docker.svg\" height=\"150\"><img src=\"https://cdn.jsdelivr.net/gh/drengskapur/docker-in-colab/assets/colab.svg\" height=\"150\"></center><br/>\n",
        "# @markdown <center><h1>Docker in Colab</h1></center><center>github.com/drengskapur/docker-in-colab<br/><br/><br/><b>udocker(\"run hello-world\")</b></center><br/>\n",
        "def udocker_init():\n",
        "    import os\n",
        "    if not os.path.exists(\"/home/user\"):\n",
        "        !pip install udocker > /dev/null\n",
        "        !udocker --allow-root install > /dev/null\n",
        "        !useradd -m user > /dev/null\n",
        "    print(f'Docker-in-Colab 1.1.0\\n')\n",
        "    print(f'Usage:     udocker(\"--help\")')\n",
        "    print(f'Examples:  https://github.com/indigo-dc/udocker?tab=readme-ov-file#examples')\n",
        "\n",
        "    def execute(command: str):\n",
        "        user_prompt = \"\\033[1;32muser@pc\\033[0m\"\n",
        "        print(f\"{user_prompt}$ udocker {command}\")\n",
        "        !su - user -c \"udocker $command\"\n",
        "\n",
        "    return execute\n",
        "\n",
        "udocker = udocker_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGX_b5JZWy0M",
        "outputId": "d296570c-b8ba-4f1f-b408-c9af2d726696"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Docker-in-Colab 1.1.0\n",
            "\n",
            "Usage:     udocker(\"--help\")\n",
            "Examples:  https://github.com/indigo-dc/udocker?tab=readme-ov-file#examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain and start the basic docker image environment.\n",
        "#docker --allow-root run --rm --runtime=nvidia --gpus all --volume ${PWD}:/TensorRT-LLM --entrypoint /bin/bash -it --workdir /TensorRT-LLM nvidia/cuda:12.1.0-devel-ubuntu22.04\n",
        "\n",
        "# Install dependencies, TensorRT-LLM requires Python 3.10\n",
        "!apt-get update && apt-get -y install python3.10 python3-pip openmpi-bin libopenmpi-dev\n",
        "\n",
        "# Install the stable version (corresponding to the cloned branch) of TensorRT-LLM.\n",
        "!pip3 install tensorrt_llm==0.8.0 -U --extra-index-url https://pypi.nvidia.com\n",
        "!/tmp/llama/7B/trt_engines/bf16/1-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqLaQnqiazhX",
        "outputId": "bf86a5be-7cf4-4cf4-8aa2-b95bc0ec8931"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ubuntu.com (91.189.91.8\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [2 InRelease 3,626 B/3,626 B 100%] [Connecting to ppa\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcontent.net (185.125.190.\r                                                                                                    \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r                                                                                                    \r0% [Waiting for headers] [Connecting to ppa.launchpadcontent.net (185.125.190.80)]\r                                                                                  \rGet:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,083 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,118 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,845 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,374 kB]\n",
            "Fetched 6,653 kB in 1s (5,597 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "openmpi-bin is already the newest version (4.1.2-2ubuntu1).\n",
            "openmpi-bin set to manually installed.\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.3).\n",
            "The following additional packages will be installed:\n",
            "  python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,677 kB of archives.\n",
            "After this operation, 8,967 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.1 [339 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.4 [1,305 kB]\n",
            "Fetched 1,677 kB in 0s (6,789 kB/s)\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-setuptools_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_22.0.2+dfsg-1ubuntu0.4_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Collecting tensorrt_llm==0.8.0\n",
            "  Downloading https://pypi.nvidia.com/tensorrt-llm/tensorrt_llm-0.8.0-cp310-cp310-linux_x86_64.whl (1126.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 GB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.25.0 (from tensorrt_llm==0.8.0)\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (1.2.1)\n",
            "Collecting colored (from tensorrt_llm==0.8.0)\n",
            "  Downloading colored-2.2.4-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cuda-python in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (12.2.1)\n",
            "Collecting diffusers==0.15.0 (from tensorrt_llm==0.8.0)\n",
            "  Downloading diffusers-0.15.0-py3-none-any.whl (851 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m851.8/851.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lark (from tensorrt_llm==0.8.0)\n",
            "  Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mpi4py (from tensorrt_llm==0.8.0)\n",
            "  Downloading mpi4py-3.1.6.tar.gz (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (1.25.2)\n",
            "Collecting onnx>=1.12.0 (from tensorrt_llm==0.8.0)\n",
            "  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting polygraphy (from tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/polygraphy/polygraphy-0.49.9-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.9/346.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (5.9.5)\n",
            "Collecting pynvml>=11.5.0 (from tensorrt_llm==0.8.0)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (0.1.99)\n",
            "Collecting tensorrt==9.2.0.post12.dev5 (from tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/tensorrt/tensorrt-9.2.0.post12.dev5.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch<=2.2.0a (from tensorrt_llm==0.8.0)\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.36.1 (from tensorrt_llm==0.8.0)\n",
            "  Downloading transformers-4.36.1-py3-none-any.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (0.43.0)\n",
            "Collecting optimum (from tensorrt_llm==0.8.0)\n",
            "  Downloading optimum-1.19.2-py3-none-any.whl (417 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.0/417.0 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate (from tensorrt_llm==0.8.0)\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting janus (from tensorrt_llm==0.8.0)\n",
            "  Downloading janus-1.0.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting nvidia-ammo~=0.7.0 (from tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-ammo/nvidia_ammo-0.7.4-cp310-cp310-linux_x86_64.whl (975 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.6/975.6 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.25.0->tensorrt_llm==0.8.0) (24.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.25.0->tensorrt_llm==0.8.0) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.25.0->tensorrt_llm==0.8.0) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.25.0->tensorrt_llm==0.8.0) (0.4.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm==0.8.0) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm==0.8.0) (3.14.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm==0.8.0) (7.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm==0.8.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm==0.8.0) (2.31.0)\n",
            "Collecting tensorrt_libs==9.2.0.post12.dev5 (from tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/tensorrt-libs/tensorrt_libs-9.2.0.post12.dev5-py2.py3-none-manylinux_2_17_x86_64.whl (1076.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 GB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorrt_bindings==9.2.0.post12.dev5 (from tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/tensorrt-bindings/tensorrt_bindings-9.2.0.post12.dev5-cp310-none-manylinux_2_17_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers==4.36.1->tensorrt_llm==0.8.0)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.1->tensorrt_llm==0.8.0) (4.66.4)\n",
            "Collecting nvidia-cuda-runtime-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.1.17-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cublas-cu12/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0) (3.3)\n",
            "Collecting onnxruntime~=1.16.1 (from nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx-graphsurgeon (from nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/onnx-graphsurgeon/onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0) (1.11.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0->tensorrt_llm==0.8.0) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<=2.2.0a->tensorrt_llm==0.8.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<=2.2.0a->tensorrt_llm==0.8.0) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<=2.2.0a->tensorrt_llm==0.8.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<=2.2.0a->tensorrt_llm==0.8.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cudnn-cu12/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cublas-cu12/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cufft-cu12/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-curand-cu12/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nccl-cu12/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.1.0 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->tensorrt_llm==0.8.0) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->tensorrt_llm==0.8.0) (2.0.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python->tensorrt_llm==0.8.0) (3.0.10)\n",
            "Collecting datasets>=2.0.0 (from evaluate->tensorrt_llm==0.8.0)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from evaluate->tensorrt_llm==0.8.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate->tensorrt_llm==0.8.0) (2.0.3)\n",
            "Collecting xxhash (from evaluate->tensorrt_llm==0.8.0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate->tensorrt_llm==0.8.0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from optimum->tensorrt_llm==0.8.0)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[sentencepiece]<4.41.0,>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum->tensorrt_llm==0.8.0) (4.40.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (3.9.5)\n",
            "Collecting huggingface-hub (from accelerate==0.25.0->tensorrt_llm==0.8.0)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime~=1.16.1->nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0) (24.3.25)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm==0.8.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm==0.8.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm==0.8.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm==0.8.0) (2024.2.2)\n",
            "INFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers[sentencepiece]<4.41.0,>=4.26.0 (from optimum->tensorrt_llm==0.8.0)\n",
            "  Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.1-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.0-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.1-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->optimum->tensorrt_llm==0.8.0)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.15.0->tensorrt_llm==0.8.0) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<=2.2.0a->tensorrt_llm==0.8.0) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->tensorrt_llm==0.8.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->tensorrt_llm==0.8.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->tensorrt_llm==0.8.0) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<=2.2.0a->tensorrt_llm==0.8.0) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate->tensorrt_llm==0.8.0) (1.16.0)\n",
            "Building wheels for collected packages: tensorrt, mpi4py\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-9.2.0.post12.dev5-py2.py3-none-any.whl size=17625 sha256=1f53e63213124dbe2532434a29c283e3ef91f5cfd2e07070e4afc7593fd6e38f\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/96/bf/028c219d3560856a5fdb8b3aec8bf01e9d485521c092a64d02\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.6-cp310-cp310-linux_x86_64.whl size=2746308 sha256=5732ebb874987abdfccf2abb7ff6b69368c35dfb8ed048f485f8f7d08bc8b666\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/ca/89/8fc1fb1c620afca13bb41c630b1f948bbf446e0aaa4b762e10\n",
            "Successfully built tensorrt mpi4py\n",
            "Installing collected packages: tensorrt_bindings, ninja, xxhash, triton, pynvml, polygraphy, onnx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mpi4py, lark, janus, humanfriendly, dill, colored, onnx-graphsurgeon, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, coloredlogs, tokenizers, tensorrt_libs, onnxruntime, nvidia-cusolver-cu12, diffusers, transformers, torch, tensorrt, datasets, nvidia-ammo, evaluate, accelerate, optimum, tensorrt_llm\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.40.2\n",
            "    Uninstalling transformers-4.40.2:\n",
            "      Successfully uninstalled transformers-4.40.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.25.0 colored-2.2.4 coloredlogs-15.0.1 datasets-2.19.1 diffusers-0.15.0 dill-0.3.8 evaluate-0.4.2 huggingface-hub-0.23.0 humanfriendly-10.0 janus-1.0.0 lark-1.1.9 mpi4py-3.1.6 multiprocess-0.70.16 ninja-1.11.1.1 nvidia-ammo-0.7.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 onnx-1.16.0 onnx-graphsurgeon-0.5.2 onnxruntime-1.16.3 optimum-1.19.2 polygraphy-0.49.9 pynvml-11.5.0 tensorrt-9.2.0.post12.dev5 tensorrt_bindings-9.2.0.post12.dev5 tensorrt_libs-9.2.0.post12.dev5 tensorrt_llm-0.8.0 tokenizers-0.15.2 torch-2.1.2 transformers-4.36.1 triton-2.1.0 xxhash-3.4.1\n",
            "/bin/bash: line 1: /tmp/llama/7B/trt_engines/bf16/1-gpu: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/TensorRT-LLM/examples/llama/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ouf0iVHDQjz",
        "outputId": "710bd069-0e3b-4968-a097-46f04c921a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.14.6 (from -r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1))\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/493.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m368.6/493.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate~=0.4.1 (from -r /content/TensorRT-LLM/examples/llama/requirements.txt (line 2))\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge_score~=0.1.2 (from -r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentencepiece~=0.1.99 in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/llama/requirements.txt (line 4)) (0.1.99)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (14.0.2)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (4.64.1)\n",
            "Collecting xxhash (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1))\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2024.2.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2024.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=f7fa0d22c6f79dd2143531a195fbe8ae671a129bc4e8b28ac65e96235dbae590\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: xxhash, dill, rouge_score, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 evaluate-0.4.2 multiprocess-0.70.15 rouge_score-0.1.2 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Llama 8B model using a single GPU and BF16.\n",
        "!python3 /content/TensorRT-LLM/examples/llama/convert_checkpoint.py --model_dir /content/Meta-Llama-3-8B-Instruct \\\n",
        "            --output_dir ./tllm_checkpoint_1gpu_bf16 \\\n",
        "            --load_model_on_cpu \\\n",
        "            --dtype bfloat16\n",
        "\n",
        "# !python convert_checkpoint.py --model_dir mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "#             --output_dir /output/mixtral-w4a16-tp2 \\\n",
        "#             --dtype bfloat16 \\\n",
        "#             --tp_size 2 \\\n",
        "#             --use_weight_only \\\n",
        "#             --weight_only_precision \\\n",
        "#             --int4 \\\n",
        "#             --moe_num_experts 8 \\\n",
        "#             --moe_top_k 2 \\\n",
        "#             --load_model_on_cpu\n",
        "\n",
        "!trtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_bf16 \\\n",
        "            --output_dir ./tmp/llama/8B/trt_engines/bf16/1-gpu \\\n",
        "            --gpt_attention_plugin bfloat16 \\\n",
        "            --gemm_plugin bfloat16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_kxyeM1IonM",
        "outputId": "3671c660-3991-4467-a7ca-b8ecb776a0fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TensorRT-LLM] TensorRT-LLM version: 0.8.00.8.0\n",
            "Loading checkpoint shards: 100% 4/4 [00:01<00:00,  2.59it/s]\n",
            "Weights loaded. Total time: 00:00:55\n",
            "Total time of converting checkpoints: 00:02:11\n",
            "[TensorRT-LLM] TensorRT-LLM version: 0.8.0[05/15/2024-18:34:16] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set lookup_plugin to None.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set lora_plugin to None.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set context_fmha to True.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set remove_input_padding to True.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set multi_block_mode to False.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set enable_xqa to True.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n",
            "[05/15/2024-18:34:16] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
            "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n",
            "[05/15/2024-18:34:17] [TRT] [I] [MemUsageChange] Init CUDA: CPU +13, GPU +0, now: CPU 258, GPU 103 (MiB)\n",
            "[05/15/2024-18:34:23] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +979, GPU +180, now: CPU 1372, GPU 283 (MiB)\n",
            "[05/15/2024-18:34:23] [TRT-LLM] [I] Set nccl_plugin to None.\n",
            "[05/15/2024-18:34:23] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
            "[05/15/2024-18:34:23] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
            "[05/15/2024-18:34:23] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
            "[TensorRT-LLM][WARNING] Fall back to unfused MHA because of unsupported head size 128 in sm_{75}.\n",
            "[TensorRT-LLM][ERROR] tensorrt_llm::common::TllmException: [TensorRT-LLM][ERROR] Assertion failed: Unsupported data type, pre SM 80 GPUs do not support bfloat16 (/home/jenkins/agent/workspace/LLM/release-0.8/L0_PostMerge/tensorrt_llm/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp:444)\n",
            "1       0x7c154e2b0803 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/libnvinfer_plugin_tensorrt_llm.so(+0x38803) [0x7c154e2b0803]\n",
            "2       0x7c154e2b0a9e /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/libnvinfer_plugin_tensorrt_llm.so(+0x38a9e) [0x7c154e2b0a9e]\n",
            "3       0x7c154e2d80eb tensorrt_llm::plugins::GPTAttentionPlugin::GPTAttentionPlugin(int, int, int, int, float, tensorrt_llm::kernels::PositionEmbeddingType, int, float, tensorrt_llm::kernels::RotaryScalingType, float, int, int, int, bool, tensorrt_llm::kernels::ContextFMHAType, bool, bool, int, bool, tensorrt_llm::kernels::AttentionMaskType, bool, int, nvinfer1::DataType, int, bool, bool, int, bool, bool, bool, bool, bool) + 219\n",
            "4       0x7c154e2d8b84 tensorrt_llm::plugins::GPTAttentionPluginCreator::createPlugin(char const*, nvinfer1::PluginFieldCollection const*) + 2644\n",
            "5       0x7c15dc96030a /usr/local/lib/python3.10/dist-packages/tensorrt_bindings/tensorrt.so(+0x16030a) [0x7c15dc96030a]\n",
            "6       0x7c15dc843433 /usr/local/lib/python3.10/dist-packages/tensorrt_bindings/tensorrt.so(+0x43433) [0x7c15dc843433]\n",
            "7       0x5794f4b7d10e /usr/bin/python3(+0x15a10e) [0x5794f4b7d10e]\n",
            "8       0x5794f4b73a7b _PyObject_MakeTpCall + 603\n",
            "9       0x5794f4b8bacb /usr/bin/python3(+0x168acb) [0x5794f4b8bacb]\n",
            "10      0x5794f4b6bcfa _PyEval_EvalFrameDefault + 24906\n",
            "11      0x5794f4b7d9fc _PyFunction_Vectorcall + 124\n",
            "12      0x5794f4b8c492 PyObject_Call + 290\n",
            "13      0x5794f4b685d7 _PyEval_EvalFrameDefault + 10791\n",
            "14      0x5794f4b7d9fc _PyFunction_Vectorcall + 124\n",
            "15      0x5794f4b8c492 PyObject_Call + 290\n",
            "16      0x5794f4b685d7 _PyEval_EvalFrameDefault + 10791\n",
            "17      0x5794f4b8b7f1 /usr/bin/python3(+0x1687f1) [0x5794f4b8b7f1]\n",
            "18      0x5794f4b8c492 PyObject_Call + 290\n",
            "19      0x5794f4b685d7 _PyEval_EvalFrameDefault + 10791\n",
            "20      0x5794f4b7d9fc _PyFunction_Vectorcall + 124\n",
            "21      0x5794f4b72cbd _PyObject_FastCallDictTstate + 365\n",
            "22      0x5794f4b8886c _PyObject_Call_Prepend + 92\n",
            "23      0x5794f4ca3700 /usr/bin/python3(+0x280700) [0x5794f4ca3700]\n",
            "24      0x5794f4b73a7b _PyObject_MakeTpCall + 603\n",
            "25      0x5794f4b6d150 _PyEval_EvalFrameDefault + 30112\n",
            "26      0x5794f4b8b7f1 /usr/bin/python3(+0x1687f1) [0x5794f4b8b7f1]\n",
            "27      0x5794f4b8c492 PyObject_Call + 290\n",
            "28      0x5794f4b685d7 _PyEval_EvalFrameDefault + 10791\n",
            "29      0x5794f4b7d9fc _PyFunction_Vectorcall + 124\n",
            "30      0x5794f4b72cbd _PyObject_FastCallDictTstate + 365\n",
            "31      0x5794f4b8886c _PyObject_Call_Prepend + 92\n",
            "32      0x5794f4ca3700 /usr/bin/python3(+0x280700) [0x5794f4ca3700]\n",
            "33      0x5794f4b8c42b PyObject_Call + 187\n",
            "34      0x5794f4b685d7 _PyEval_EvalFrameDefault + 10791\n",
            "35      0x5794f4b8b7f1 /usr/bin/python3(+0x1687f1) [0x5794f4b8b7f1]\n",
            "36      0x5794f4b6753c _PyEval_EvalFrameDefault + 6540\n",
            "37      0x5794f4b8b7f1 /usr/bin/python3(+0x1687f1) [0x5794f4b8b7f1]\n",
            "38      0x5794f4b8c492 PyObject_Call + 290\n",
            "39      0x5794f4b685d7 _PyEval_EvalFrameDefault + 10791\n",
            "40      0x5794f4b8b7f1 /usr/bin/python3(+0x1687f1) [0x5794f4b8b7f1]\n",
            "41      0x5794f4b8c492 PyObject_Call + 290\n",
            "42      0x5794f4b685d7 _PyEval_EvalFrameDefault + 10791\n",
            "43      0x5794f4b7d9fc _PyFunction_Vectorcall + 124\n",
            "44      0x5794f4b72cbd _PyObject_FastCallDictTstate + 365\n",
            "45      0x5794f4b8886c _PyObject_Call_Prepend + 92\n",
            "46      0x5794f4ca3700 /usr/bin/python3(+0x280700) [0x5794f4ca3700]\n",
            "47      0x5794f4b8c42b PyObject_Call + 187\n",
            "48      0x5794f4b685d7 _PyEval_EvalFrameDefault + 10791\n",
            "49      0x5794f4b7d9fc _PyFunction_Vectorcall + 124\n",
            "50      0x5794f4b6626d _PyEval_EvalFrameDefault + 1725\n",
            "51      0x5794f4b7d9fc _PyFunction_Vectorcall + 124\n",
            "52      0x5794f4b8c492 PyObject_Call + 290\n",
            "53      0x5794f4b685d7 _PyEval_EvalFrameDefault + 10791\n",
            "54      0x5794f4b7d9fc _PyFunction_Vectorcall + 124\n",
            "55      0x5794f4b8c492 PyObject_Call + 290\n",
            "56      0x5794f4b685d7 _PyEval_EvalFrameDefault + 10791\n",
            "57      0x5794f4b7d9fc _PyFunction_Vectorcall + 124\n",
            "58      0x5794f4b8c492 PyObject_Call + 290\n",
            "59      0x5794f4b685d7 _PyEval_EvalFrameDefault + 10791\n",
            "60      0x5794f4b7d9fc _PyFunction_Vectorcall + 124\n",
            "61      0x5794f4b6626d _PyEval_EvalFrameDefault + 1725\n",
            "62      0x5794f4b629c6 /usr/bin/python3(+0x13f9c6) [0x5794f4b629c6]\n",
            "63      0x5794f4c58256 PyEval_EvalCode + 134\n",
            "64      0x5794f4c83108 /usr/bin/python3(+0x260108) [0x5794f4c83108]\n",
            "65      0x5794f4c7c9cb /usr/bin/python3(+0x2599cb) [0x5794f4c7c9cb]\n",
            "66      0x5794f4c82e55 /usr/bin/python3(+0x25fe55) [0x5794f4c82e55]\n",
            "67      0x5794f4c82338 _PyRun_SimpleFileObject + 424\n",
            "68      0x5794f4c81f83 _PyRun_AnyFileObject + 67\n",
            "69      0x5794f4c74a5e Py_RunMain + 702\n",
            "70      0x5794f4c4b02d Py_BytesMain + 45\n",
            "71      0x7c174d1dfd90 /lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x7c174d1dfd90]\n",
            "72      0x7c174d1dfe40 __libc_start_main + 128\n",
            "73      0x5794f4c4af25 _start + 37\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/trtllm-build\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 497, in main\n",
            "    parallel_build(source, build_config, args.output_dir, workers,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 420, in parallel_build\n",
            "    passed = build_and_save(rank, rank % workers, ckpt_dir,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 392, in build_and_save\n",
            "    engine = build(build_config,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 282, in build\n",
            "    return build_model(model, build_config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 198, in build_model\n",
            "    model(**inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/module.py\", line 40, in __call__\n",
            "    output = self.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/modeling_utils.py\", line 498, in forward\n",
            "    hidden_states = self.transformer.forward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/llama/model.py\", line 202, in forward\n",
            "    hidden_states = self.layers.forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/modeling_utils.py\", line 255, in forward\n",
            "    hidden_states = layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/module.py\", line 40, in __call__\n",
            "    output = self.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/llama/model.py\", line 116, in forward\n",
            "    attention_output = self.attention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/module.py\", line 40, in __call__\n",
            "    output = self.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/layers/attention.py\", line 742, in forward\n",
            "    context, past_key_value = gpt_attention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/graph_rewriting.py\", line 561, in wrapper\n",
            "    outs = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/functional.py\", line 3813, in gpt_attention\n",
            "    layer = default_trtnet().add_plugin_v2(plug_inputs, attn_plug)\n",
            "TypeError: add_plugin_v2(): incompatible function arguments. The following argument types are supported:\n",
            "    1. (self: tensorrt_bindings.tensorrt.INetworkDefinition, inputs: List[tensorrt_bindings.tensorrt.ITensor], plugin: tensorrt_bindings.tensorrt.IPluginV2) -> tensorrt_bindings.tensorrt.IPluginV2Layer\n",
            "\n",
            "Invoked with: <tensorrt_bindings.tensorrt.INetworkDefinition object at 0x7c15c5171130>, [<tensorrt_bindings.tensorrt.ITensor object at 0x7c154ca90670>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c1554448e70>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c15544493f0>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c1554449c70>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c154ca62b30>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c1554449670>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c154ca62d30>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c1554449130>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c1554406ef0>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c1554407170>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c1554449930>], None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert_checkpoint.py --model_dir mistralai/Mixtral-8x7B-Instruct-v0.1 --output_dir /output/mixtral-w4a16-tp2/ --dtype bfloat16 --tp_size 2 --use_weight_only --weight_only_precision int4 --moe_num_experts 8 --moe_top_k 2 --load_model_on_cpu"
      ],
      "metadata": {
        "id": "0BQCxM38P8qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ND1X-A8Vg0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}